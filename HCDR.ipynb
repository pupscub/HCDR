{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "551ce207-0976-48c3-9242-fac3e6bdf527",
        "_uuid": "66406036d8dd7a0071295d1aee64f13bffc44e3a",
        "id": "xWW_1kKhpVpO"
      },
      "source": [
        "# Introduction: Home Credit Default Risk Competition\n",
        "\n",
        "\n",
        "# Data\n",
        "\n",
        "The data is provided by [Home Credit](http://www.homecredit.net/about-us.aspx)\n",
        "\n",
        "There are 7 different sources of data:\n",
        "\n",
        "* application_train/application_test: the main training and testing data with information about each loan application at Home Credit. Every loan has its own row and is identified by the feature `SK_ID_CURR`. The training application data comes with the `TARGET` indicating 0: the loan was repaid or 1: the loan was not repaid.\n",
        "* bureau: data concerning client's previous credits from other financial institutions. Each previous credit has its own row in bureau, but one loan in the application data can have multiple previous credits.\n",
        "* bureau_balance: monthly data about the previous credits in bureau. Each row is one month of a previous credit, and a single previous credit can have multiple rows, one for each month of the credit length.\n",
        "* previous_application: previous applications for loans at Home Credit of clients who have loans in the application data. Each current loan in the application data can have multiple previous loans. Each previous application has one row and is identified by the feature `SK_ID_PREV`.\n",
        "* POS_CASH_BALANCE: monthly data about previous point of sale or cash loans clients have had with Home Credit. Each row is one month of a previous point of sale or cash loan, and a single previous loan can have many rows.\n",
        "* credit_card_balance: monthly data about previous credit cards clients have had with Home Credit. Each row is one month of a credit card balance, and a single credit card can have many rows.\n",
        "* installments_payment: payment history for previous loans at Home Credit. There is one row for every made payment and one row for every missed payment.\n",
        "\n",
        "\n",
        "##### Metric: ROC AUC \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:36:36.891871Z",
          "iopub.status.busy": "2023-11-15T16:36:36.891502Z",
          "iopub.status.idle": "2023-11-15T16:36:36.897392Z",
          "shell.execute_reply": "2023-11-15T16:36:36.896388Z",
          "shell.execute_reply.started": "2023-11-15T16:36:36.891811Z"
        },
        "id": "2oRUH2kMpVpR",
        "outputId": "c866e0bf-96e5-4850-f774-db28fa1f1471",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# numpy and pandas for data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# sklearn preprocessing for dealing with categorical variables\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# File system manangement\n",
        "import os\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# matplotlib and seaborn for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import lightgbm as lgb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a5e67831-4751-4f11-8e07-527e3e092671",
        "_uuid": "ded520f73b9e94ed47ac2e994a5fb1bcb9093d0f",
        "id": "AyzSFUwJpVpS"
      },
      "source": [
        "## Read in Data\n",
        "\n",
        "Using only Application_Train and Application_test in this half of the notebook "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw-aN9GDqAhC",
        "outputId": "9ccfd1b1-0908-43ce-e0fb-87606d3ccfa3"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/My Drive/KaggleTraining/Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:36:37.073280Z",
          "iopub.status.busy": "2023-11-15T16:36:37.072491Z",
          "iopub.status.idle": "2023-11-15T16:36:42.891546Z",
          "shell.execute_reply": "2023-11-15T16:36:42.890717Z",
          "shell.execute_reply.started": "2023-11-15T16:36:37.073197Z"
        },
        "id": "x7c89lVKpVpS",
        "outputId": "d6a38667-8aa2-4e16-cf97-ce85a4ed6111",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Training data\n",
        "app_train = pd.read_csv('application_train.csv')\n",
        "print('Training data shape: ', app_train.shape)\n",
        "app_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "4695541966d3d29e8a7a8975b072d01caff1631d",
        "id": "MzRe7oDIpVpT"
      },
      "source": [
        "The training data has 307511 observations (each one a separate loan) and 122 features (variables) including the `TARGET` (the label we want to predict)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d077aee0-5271-440e-bc07-6087eab40b74",
        "_uuid": "cbd1c4111df6f07bc0d479b51f50895e728b717a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:36:42.893144Z",
          "iopub.status.busy": "2023-11-15T16:36:42.892704Z",
          "iopub.status.idle": "2023-11-15T16:36:43.930894Z",
          "shell.execute_reply": "2023-11-15T16:36:43.929744Z",
          "shell.execute_reply.started": "2023-11-15T16:36:42.893098Z"
        },
        "id": "9tOm8BeGpVpT",
        "outputId": "3a9af58d-be32-4007-d5a6-a825310d3e60",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Testing data features\n",
        "app_test = pd.read_csv('application_test.csv')\n",
        "print('Testing data shape: ', app_test.shape)\n",
        "app_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "e351f02c8a5886756507a2d4f1ddba4791220f12",
        "id": "_U7IfM0WpVpT"
      },
      "source": [
        "The test set is considerably smaller and lacks a `TARGET` column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "0b1a02afd367d1c4ee3a3a936382ca42fb921b9d",
        "id": "0XsF__xDpVpT"
      },
      "source": [
        "# Exploratory Data Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "23b20e53-3484-4c4b-bec9-2d8ac2ac918d",
        "_uuid": "7c006a09627df1333c557dc11a09f372bde34dda",
        "id": "PAxjzs0epVpT"
      },
      "source": [
        "## Examine the Distribution of the Target Column\n",
        "\n",
        "The target is what we are asked to predict: either a 0 for the loan was repaid on time, or a 1 indicating the client had payment difficulties. We can first examine the number of loans falling into each category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5fb6ab16-1b38-4ecf-8123-e48c7c061773",
        "_uuid": "2163ca09678b53dbe88388ccbc7d0e0f7d6c6230",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:36:43.932463Z",
          "iopub.status.busy": "2023-11-15T16:36:43.932165Z",
          "iopub.status.idle": "2023-11-15T16:36:43.947026Z",
          "shell.execute_reply": "2023-11-15T16:36:43.945886Z",
          "shell.execute_reply.started": "2023-11-15T16:36:43.932406Z"
        },
        "id": "M9jon7SFpVpT",
        "outputId": "1d96051f-6f72-4105-a63a-ca1adaafce41",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "app_train['TARGET'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0e93c1e2-f6b8-4a0b-82b6-7dad8df56048",
        "_uuid": "1b2611fb3cf392023c3f40fd2f7b96f56f5dee7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:36:43.949156Z",
          "iopub.status.busy": "2023-11-15T16:36:43.948802Z",
          "iopub.status.idle": "2023-11-15T16:36:44.199969Z",
          "shell.execute_reply": "2023-11-15T16:36:44.198832Z",
          "shell.execute_reply.started": "2023-11-15T16:36:43.949082Z"
        },
        "id": "ytVyIhjepVpU",
        "outputId": "92ede2a9-de44-4f1c-fae5-149b1a1fa1e4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "app_train['TARGET'].astype(int).plot.hist();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "507ec6b1-99d0-4324-a3ed-bdea2f916227",
        "_uuid": "58851dfef481f32b3026e89b086534ea3683440d",
        "id": "RJQFF-WIpVpU"
      },
      "source": [
        "## Missing Values\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fc4c675f-e4a1-4e4f-9ece-3c59e5c8f7fd",
        "_uuid": "7a2f5c72c45fa04d9fa95e8051ae595be806e9a2",
        "execution": {
          "iopub.execute_input": "2023-11-15T16:36:44.202226Z",
          "iopub.status.busy": "2023-11-15T16:36:44.201588Z",
          "iopub.status.idle": "2023-11-15T16:36:44.213123Z",
          "shell.execute_reply": "2023-11-15T16:36:44.212118Z",
          "shell.execute_reply.started": "2023-11-15T16:36:44.202151Z"
        },
        "id": "9_L0A2P3pVpU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Function to calculate missing values by column# Funct\n",
        "def missing_values_table(df):\n",
        "        # Total missing values\n",
        "        mis_val = df.isnull().sum()\n",
        "\n",
        "        # Percentage of missing values\n",
        "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
        "\n",
        "        # Make a table with the results\n",
        "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
        "\n",
        "        # Rename the columns\n",
        "        mis_val_table_ren_columns = mis_val_table.rename(\n",
        "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
        "\n",
        "        # Sort the table by percentage of missing descending\n",
        "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
        "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
        "        '% of Total Values', ascending=False).round(1)\n",
        "\n",
        "        # Print some summary information\n",
        "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"\n",
        "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
        "              \" columns that have missing values.\")\n",
        "\n",
        "        # Return the dataframe with missing information\n",
        "        return mis_val_table_ren_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_missing_columns(train, test, threshold = 90):\n",
        "    # Calculate missing stats for train and test (remember to calculate a percent!)\n",
        "    train_miss = pd.DataFrame(train.isnull().sum())\n",
        "    train_miss['percent'] = 100 * train_miss[0] / len(train)\n",
        "    \n",
        "    test_miss = pd.DataFrame(test.isnull().sum())\n",
        "    test_miss['percent'] = 100 * test_miss[0] / len(test)\n",
        "    \n",
        "    # list of missing columns for train and test\n",
        "    missing_train_columns = list(train_miss.index[train_miss['percent'] > threshold])\n",
        "    missing_test_columns = list(test_miss.index[test_miss['percent'] > threshold])\n",
        "    \n",
        "    # Combine the two lists together\n",
        "    missing_columns = list(set(missing_train_columns + missing_test_columns))\n",
        "    \n",
        "    # Print information\n",
        "    print('There are %d columns with greater than %d%% missing values.' % (len(missing_columns), threshold))\n",
        "    \n",
        "    # Drop the missing columns and return\n",
        "    train = train.drop(columns = missing_columns)\n",
        "    test = test.drop(columns = missing_columns)\n",
        "    \n",
        "    return train, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "786881f0-235e-441c-8319-f715a3b7d920",
        "_uuid": "98b0a82a3009b8f6d0bc718a2e1eaba779b4ace9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:36:44.215627Z",
          "iopub.status.busy": "2023-11-15T16:36:44.214859Z",
          "iopub.status.idle": "2023-11-15T16:36:46.457412Z",
          "shell.execute_reply": "2023-11-15T16:36:46.456427Z",
          "shell.execute_reply.started": "2023-11-15T16:36:44.215549Z"
        },
        "id": "gOp9jgv6pVpU",
        "outputId": "535a787c-9afb-4c04-9835-65dc3a3899a4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Missing values statistics\n",
        "missing_values = missing_values_table(app_train)\n",
        "missing_values.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "0672e40c3ab75a7901c0de35d248b322a227dc7f",
        "id": "bb6w98UlpVpV"
      },
      "source": [
        "## Column Types\n",
        " Number of columns of each data type. `int64` and `float64` are numeric variables ([which can be either discrete or continuous](https://stats.stackexchange.com/questions/206/what-is-the-difference-between-discrete-data-and-continuous-data)). `object` columns contain strings and are  [categorical features.](http://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/what-are-categorical-discrete-and-continuous-variables/) ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "a03caadd76fa32f4b193e52467d4f39f2145d7b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:36:46.458968Z",
          "iopub.status.busy": "2023-11-15T16:36:46.458688Z",
          "iopub.status.idle": "2023-11-15T16:36:46.468573Z",
          "shell.execute_reply": "2023-11-15T16:36:46.466684Z",
          "shell.execute_reply.started": "2023-11-15T16:36:46.458921Z"
        },
        "id": "Hlz9-vZlpVpV",
        "outputId": "89876417-7084-426d-e8e8-ffc09a530bb9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Number of each type of column\n",
        "app_train.dtypes.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "5859303c9acc63f7ff7acce063a9cd022a6d38cd",
        "id": "irkeHsT3pVpV"
      },
      "source": [
        "Number of unique entries in each of the `object` (categorical) columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "2d021eda10939a19b141292d34491b357acd201a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:36:46.471449Z",
          "iopub.status.busy": "2023-11-15T16:36:46.471005Z",
          "iopub.status.idle": "2023-11-15T16:36:46.917230Z",
          "shell.execute_reply": "2023-11-15T16:36:46.915588Z",
          "shell.execute_reply.started": "2023-11-15T16:36:46.471378Z"
        },
        "id": "Aplsq1-epVpV",
        "outputId": "012261fd-5255-428e-ffe1-7540629bc072",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Number of unique classes in each object column\n",
        "app_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "86d1b309-5524-4298-b873-2c1c09eddec6",
        "_uuid": "1b49e667293daabffd8a4b2b6d02cf44bf6a3ba8",
        "id": "LkPxQ0E8pVpV"
      },
      "source": [
        "## Encoding Categorical Variables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "95627792-157e-457a-88a8-3b3875c7e1d5",
        "_uuid": "46f5bf9a6de52e270aa911ffd895e704da5426ec",
        "id": "XoglusNcpVpW"
      },
      "source": [
        "### Label Encoding and One-Hot Encoding\n",
        "\n",
        "policy described above: for any categorical variable (`dtype == object`) with 2 unique categories, we will use label encoding, and for any categorical variable with more than 2 unique categories, we will use one-hot encoding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "70641d4d-1075-4837-8972-e58d70d8f242",
        "_uuid": "ddfaae5c3dcc7ec6bb47a2dffc10d364e8d25355",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:36:46.919841Z",
          "iopub.status.busy": "2023-11-15T16:36:46.919058Z",
          "iopub.status.idle": "2023-11-15T16:36:47.680257Z",
          "shell.execute_reply": "2023-11-15T16:36:47.679280Z",
          "shell.execute_reply.started": "2023-11-15T16:36:46.919463Z"
        },
        "id": "2vFXVQ3cpVpW",
        "outputId": "3a11a810-1780-48cb-98fe-b375366892dd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Create a label encoder object\n",
        "le = LabelEncoder()\n",
        "le_count = 0\n",
        "\n",
        "# Iterate through the columns\n",
        "for col in app_train:\n",
        "    if app_train[col].dtype == 'object':\n",
        "        # If 2 or fewer unique categories\n",
        "        if len(list(app_train[col].unique())) <= 2:\n",
        "            # Train on the training data\n",
        "            le.fit(app_train[col])\n",
        "            # Transform both training and testing data\n",
        "            app_train[col] = le.transform(app_train[col])\n",
        "            app_test[col] = le.transform(app_test[col])\n",
        "\n",
        "            # Keep track of how many columns were label encoded\n",
        "            le_count += 1\n",
        "\n",
        "print('%d columns were label encoded.' % le_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0851773b-39fd-4cf0-9a66-e30adeef3e57",
        "_uuid": "6796c6dc793a08e162b6e20c6f185ef37bdf51f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:36:47.681759Z",
          "iopub.status.busy": "2023-11-15T16:36:47.681481Z",
          "iopub.status.idle": "2023-11-15T16:36:49.099946Z",
          "shell.execute_reply": "2023-11-15T16:36:49.098975Z",
          "shell.execute_reply.started": "2023-11-15T16:36:47.681711Z"
        },
        "id": "5vJsyCyPpVpW",
        "outputId": "95dcb94c-70d7-4950-d8a2-c26015aeacf8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# one-hot encoding of categorical variables\n",
        "app_train = pd.get_dummies(app_train)\n",
        "app_test = pd.get_dummies(app_test)\n",
        "\n",
        "print('Training Features shape: ', app_train.shape)\n",
        "print('Testing Features shape: ', app_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "61d910b5-84f5-4655-bd8a-d29672c13741",
        "_uuid": "1b2c4198638ec8e5155097d112249de8754eb5c0",
        "id": "UIyzzH0OpVpW"
      },
      "source": [
        "### Aligning Training and Testing Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d99ca215-e893-490c-a6a4-83f3e8a067b3",
        "_uuid": "e0d12a13cb95521c19b10d8829e8abe2b1118396",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:36:49.101414Z",
          "iopub.status.busy": "2023-11-15T16:36:49.101151Z",
          "iopub.status.idle": "2023-11-15T16:36:49.930045Z",
          "shell.execute_reply": "2023-11-15T16:36:49.929012Z",
          "shell.execute_reply.started": "2023-11-15T16:36:49.101368Z"
        },
        "id": "5CwaaDSmpVpW",
        "outputId": "447ddeeb-a156-459a-d739-1c2eea5a8e10",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_labels = app_train['TARGET']\n",
        "\n",
        "# Align the training and testing data, keep only columns present in both dataframes\n",
        "app_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n",
        "\n",
        "# Add the target back in\n",
        "app_train['TARGET'] = train_labels\n",
        "print('Training Features shape: ', app_train.shape)\n",
        "print('Testing Features shape: ', app_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "13918211-0e6b-4d72-955b-f997db19eea2",
        "_uuid": "4d7c8dd1d5bb5a0ef84cb78e6bff927249e62145",
        "id": "DP0fn6NHpVpW"
      },
      "source": [
        "## Exploratory Data Analysis\n",
        "\n",
        "### Anomalies\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "a60be93c2d7d63855e6d65c1109f408ad85da134",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:36:49.931582Z",
          "iopub.status.busy": "2023-11-15T16:36:49.931269Z",
          "iopub.status.idle": "2023-11-15T16:36:49.984236Z",
          "shell.execute_reply": "2023-11-15T16:36:49.983187Z",
          "shell.execute_reply.started": "2023-11-15T16:36:49.931532Z"
        },
        "id": "OgMt9XvhpVpW",
        "outputId": "8dbd2d32-d14b-4f17-f046-f8969186f200",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "(app_train['DAYS_BIRTH'] / -365).describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "acb37a3e3f2e0b2fd581259788b9255398314157",
        "id": "eiY_vk0YpVpX"
      },
      "source": [
        "Those ages look reasonable. There are no outliers for the age on either the high or low end. How about the days of employment?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "600c59dd5d970d3ccfea3a6af0036d85958adc91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:36:49.986110Z",
          "iopub.status.busy": "2023-11-15T16:36:49.985784Z",
          "iopub.status.idle": "2023-11-15T16:36:50.020901Z",
          "shell.execute_reply": "2023-11-15T16:36:50.019853Z",
          "shell.execute_reply.started": "2023-11-15T16:36:49.986055Z"
        },
        "id": "0AjXcGm-pVpX",
        "outputId": "af86a742-6401-4371-bd24-63668ec1392b",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "app_train['DAYS_EMPLOYED'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "2878bfb3a2be4554f33e03e1a04d4c1978b52a08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:36:50.022421Z",
          "iopub.status.busy": "2023-11-15T16:36:50.022136Z",
          "iopub.status.idle": "2023-11-15T16:36:50.323229Z",
          "shell.execute_reply": "2023-11-15T16:36:50.322095Z",
          "shell.execute_reply.started": "2023-11-15T16:36:50.022371Z"
        },
        "id": "4EZupG8upVpY",
        "outputId": "fa422094-7c3d-43cb-db56-4aba23b2a2de",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram')\n",
        "plt.xlabel('Days Employment')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "67ea87d9ef6974b1780a7db1eefd13f90f81b5be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:36:50.325590Z",
          "iopub.status.busy": "2023-11-15T16:36:50.324897Z",
          "iopub.status.idle": "2023-11-15T16:36:50.913978Z",
          "shell.execute_reply": "2023-11-15T16:36:50.913231Z",
          "shell.execute_reply.started": "2023-11-15T16:36:50.325517Z"
        },
        "id": "LjPCCs65pVpZ",
        "outputId": "874d151b-d1ed-4319-c6f4-401d9068627e",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\n",
        "non_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\n",
        "print('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\n",
        "print('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\n",
        "print('There are %d anomalous days of employment' % len(anom))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "e23ec3cb89428f3dd994b572f718cc729740cfab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:36:50.915534Z",
          "iopub.status.busy": "2023-11-15T16:36:50.915120Z",
          "iopub.status.idle": "2023-11-15T16:36:51.314079Z",
          "shell.execute_reply": "2023-11-15T16:36:51.313034Z",
          "shell.execute_reply.started": "2023-11-15T16:36:50.915481Z"
        },
        "id": "HnNFmi-KpVpZ",
        "outputId": "99faad48-3d2d-4262-a294-4e7f12a70fa3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Create an anomalous flag column\n",
        "app_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n",
        "\n",
        "# Replace the anomalous values with nan\n",
        "app_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
        "\n",
        "app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram')\n",
        "plt.xlabel('Days Employment')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "a0d7c77b2adecaa878f39cf86ffddcfbbe51a190",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:36:51.316270Z",
          "iopub.status.busy": "2023-11-15T16:36:51.315628Z",
          "iopub.status.idle": "2023-11-15T16:36:51.338736Z",
          "shell.execute_reply": "2023-11-15T16:36:51.337346Z",
          "shell.execute_reply.started": "2023-11-15T16:36:51.316204Z"
        },
        "id": "l_KEE0eOpVpa",
        "outputId": "67090627-6fda-4fb1-a32a-b8545c4823e5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\n",
        "app_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n",
        "\n",
        "print('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "fd656b392faad3b34ecfa448b55ad03e75449e0a",
        "id": "24TzY_2qpVpa"
      },
      "source": [
        "### Correlations\n",
        "\n",
        "* .00-.19 “very weak”\n",
        "*  .20-.39 “weak”\n",
        "*  .40-.59 “moderate”\n",
        "*  .60-.79 “strong”\n",
        "* .80-1.0 “very strong”\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "02acdb8d-d95f-41b9-8ad1-e2b6cb26f398",
        "_uuid": "d39d15d64db1f2c9015c6f542911ef9a9cac119e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:36:51.348923Z",
          "iopub.status.busy": "2023-11-15T16:36:51.345074Z",
          "iopub.status.idle": "2023-11-15T16:37:58.970953Z",
          "shell.execute_reply": "2023-11-15T16:37:58.969837Z",
          "shell.execute_reply.started": "2023-11-15T16:36:51.348786Z"
        },
        "id": "2wOQd_-EpVpa",
        "outputId": "10c619a7-e8ba-4ad3-8c04-7fafb447a24d",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Find correlations with the target and sort\n",
        "correlations = app_train.corr()['TARGET'].sort_values()\n",
        "\n",
        "# Display correlations\n",
        "print('Most Positive Correlations:\\n', correlations.tail(15))\n",
        "print('\\nMost Negative Correlations:\\n', correlations.head(15))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0f7b1cfb-9e5c-4720-9618-ad326940f3f3",
        "_uuid": "c1b831b6d1c3221efb123fbc1a4882aa1f598ec0",
        "id": "CYzBYGYPpVpb"
      },
      "source": [
        "### Effect of Age on Repayment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b0ab583c-dfbb-4ff7-80e5-d747fc408499",
        "_uuid": "f705c7aa49486ec3bf119c4edc4e4af58861b88d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:37:58.972534Z",
          "iopub.status.busy": "2023-11-15T16:37:58.972246Z",
          "iopub.status.idle": "2023-11-15T16:37:58.990431Z",
          "shell.execute_reply": "2023-11-15T16:37:58.989521Z",
          "shell.execute_reply.started": "2023-11-15T16:37:58.972480Z"
        },
        "id": "o3YqrZ4ppVpc",
        "outputId": "2a1f5483-eba1-47df-c27a-0edc191cbc7b",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Find the correlation of the positive days since birth and target\n",
        "app_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\n",
        "app_train['DAYS_BIRTH'].corr(app_train['TARGET'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "35e36393-e388-488e-ba7a-7473169d3e6f",
        "_uuid": "739226c4594130d6aabeb25ffb8742c37657d7a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:37:58.992468Z",
          "iopub.status.busy": "2023-11-15T16:37:58.992113Z",
          "iopub.status.idle": "2023-11-15T16:37:59.309086Z",
          "shell.execute_reply": "2023-11-15T16:37:59.307915Z",
          "shell.execute_reply.started": "2023-11-15T16:37:58.992396Z"
        },
        "id": "Spd9jd0GpVpc",
        "outputId": "46daa879-3188-4962-9fe3-ab14c961f269",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Set the style of plots\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "# Plot the distribution of ages in years\n",
        "plt.hist(app_train['DAYS_BIRTH'] / 365, edgecolor = 'k', bins = 25)\n",
        "plt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3982a18f-2731-4bb2-80c9-831b2377421f",
        "_uuid": "2e045e65f048789b577477356df4337c9e5e2087",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:37:59.311823Z",
          "iopub.status.busy": "2023-11-15T16:37:59.311055Z",
          "iopub.status.idle": "2023-11-15T16:37:59.763953Z",
          "shell.execute_reply": "2023-11-15T16:37:59.762896Z",
          "shell.execute_reply.started": "2023-11-15T16:37:59.311737Z"
        },
        "id": "D8BM886TpVpc",
        "outputId": "ced1d0e4-031b-4983-e06c-a87340501748",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (10, 8))\n",
        "\n",
        "# KDE plot of loans that were repaid on time\n",
        "sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label = 'target == 0')\n",
        "\n",
        "# KDE plot of loans which were not repaid on time\n",
        "sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label = 'target == 1')\n",
        "\n",
        "# Labeling of plot\n",
        "plt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4296e926-7245-40df-bb0a-f6e59d8e566a",
        "_uuid": "6c50572f095bff250bfed1993e2c53118277b5dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:37:59.766039Z",
          "iopub.status.busy": "2023-11-15T16:37:59.765640Z",
          "iopub.status.idle": "2023-11-15T16:38:00.021097Z",
          "shell.execute_reply": "2023-11-15T16:38:00.020136Z",
          "shell.execute_reply.started": "2023-11-15T16:37:59.765966Z"
        },
        "id": "GBpR4Xw1pVpc",
        "outputId": "91ec88fd-456e-4f58-e7a8-2be25156db2d",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Age information into a separate dataframe\n",
        "age_data = app_train[['TARGET', 'DAYS_BIRTH']]\n",
        "age_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n",
        "\n",
        "# Bin the age data\n",
        "age_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\n",
        "age_data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "18873d6b-3877-4c77-830e-0f3e10e5e7fb",
        "_uuid": "7082483e5fd9114856926de28968e5ae0b478b36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:38:00.023298Z",
          "iopub.status.busy": "2023-11-15T16:38:00.022624Z",
          "iopub.status.idle": "2023-11-15T16:38:00.055580Z",
          "shell.execute_reply": "2023-11-15T16:38:00.054610Z",
          "shell.execute_reply.started": "2023-11-15T16:38:00.023231Z"
        },
        "id": "M82yBYSDpVpd",
        "outputId": "8ee549cd-807d-43ae-d668-15d4e897c45d",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Group by the bin and calculate averages\n",
        "age_groups  = age_data.groupby('YEARS_BINNED').mean()\n",
        "age_groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "004d1021-d73f-4356-9ef8-0464c95d1708",
        "_uuid": "823b5032f472b05ce079ae5a7680389f31ddd8b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 863
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:38:00.057048Z",
          "iopub.status.busy": "2023-11-15T16:38:00.056633Z",
          "iopub.status.idle": "2023-11-15T16:38:00.416210Z",
          "shell.execute_reply": "2023-11-15T16:38:00.415122Z",
          "shell.execute_reply.started": "2023-11-15T16:38:00.057001Z"
        },
        "id": "8hBCT7MNpVpd",
        "outputId": "526aaea0-753a-4001-e526-50c406070a52",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (8, 8))\n",
        "\n",
        "# Graph the age bins and the average of the target as a bar plot\n",
        "plt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n",
        "\n",
        "# Plot labeling\n",
        "plt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\n",
        "plt.title('Failure to Repay by Age Group');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4749204f-ec63-4eeb-8d25-9c80967348f1",
        "_uuid": "43a3bb87bdaa65509e9dc887492239ae06cd1c77",
        "id": "H0XXZcnypVpd"
      },
      "source": [
        "### Exterior Sources\n",
        "\n",
        "The 3 variables with the strongest negative correlations with the target are `EXT_SOURCE_1`, `EXT_SOURCE_2`, and `EXT_SOURCE_3`.\n",
        "these features represent a \"normalized score from external data source\".\n",
        " correlations of the `EXT_SOURCE` features with the target and with each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e2ab3b7f-3a53-4495-a1de-31ad287f032a",
        "_uuid": "6197819149feaff75176e64e54c65ea6be3864fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:38:00.418788Z",
          "iopub.status.busy": "2023-11-15T16:38:00.418083Z",
          "iopub.status.idle": "2023-11-15T16:38:00.492072Z",
          "shell.execute_reply": "2023-11-15T16:38:00.491232Z",
          "shell.execute_reply.started": "2023-11-15T16:38:00.418727Z"
        },
        "id": "G0DEUefXpVpd",
        "outputId": "a4820119-6dce-4acc-d0ef-1a0c89afacaa",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Extract the EXT_SOURCE variables and show correlations\n",
        "ext_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
        "ext_data_corrs = ext_data.corr()\n",
        "ext_data_corrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0479863d-cfa9-47ab-83e6-7d7877e3e939",
        "_uuid": "20b21a6b4e15a726c29596abeb01346dc416729c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:38:00.493453Z",
          "iopub.status.busy": "2023-11-15T16:38:00.493050Z",
          "iopub.status.idle": "2023-11-15T16:38:00.903175Z",
          "shell.execute_reply": "2023-11-15T16:38:00.902047Z",
          "shell.execute_reply.started": "2023-11-15T16:38:00.493372Z"
        },
        "id": "avNBoU5fpVpd",
        "outputId": "529d12ea-19c3-4857-9b5c-f3c564a76b2b",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (8, 6))\n",
        "\n",
        "# Heatmap of correlations\n",
        "sns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\n",
        "plt.title('Correlation Heatmap');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "78bd5acc-003d-4795-a57a-a6c4fc9c8c5f",
        "_uuid": "6a592aa7c01858b268489ccb8fd00690cd26cd58",
        "id": "xZeROpL8pVpd"
      },
      "source": [
        "All three `EXT_SOURCE` featureshave negative correlations with the target, indicating that as the value of the `EXT_SOURCE` increases, the client is more likely to repay the loan. We can also see that `DAYS_BIRTH` is positively correlated with `EXT_SOURCE_1` indicating that maybe one of the factors in this score is the client age."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5e2b6507-96d1-4f96-964f-d8241e321f09",
        "_uuid": "49afab6b3790abcc2dea04c483f462f39e536503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:38:00.905940Z",
          "iopub.status.busy": "2023-11-15T16:38:00.905047Z",
          "iopub.status.idle": "2023-11-15T16:38:01.898586Z",
          "shell.execute_reply": "2023-11-15T16:38:01.897866Z",
          "shell.execute_reply.started": "2023-11-15T16:38:00.905849Z"
        },
        "id": "6bW3WpIbpVpd",
        "outputId": "9a511bd7-937f-4f24-95bb-a4407658c32b",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (10, 12))\n",
        "\n",
        "# iterate through the sources\n",
        "for i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n",
        "\n",
        "    # create a new subplot for each source\n",
        "    plt.subplot(3, 1, i + 1)\n",
        "    # plot repaid loans\n",
        "    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n",
        "    # plot loans that were not repaid\n",
        "    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n",
        "\n",
        "    # Label the plots\n",
        "    plt.title('Distribution of %s by Target Value' % source)\n",
        "    plt.xlabel('%s' % source); plt.ylabel('Density');\n",
        "\n",
        "plt.tight_layout(h_pad = 2.5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "53f486249f8afec0496d3de25120e57d956c2eb7",
        "id": "rGUfGbRrpVpd"
      },
      "source": [
        "## Pairs Plot\n",
        "\n",
        "As a final exploratory plot, we can make a pairs plot of the `EXT_SOURCE` variables and the `DAYS_BIRTH` variable. The [Pairs Plot](https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166) is a great exploration tool because it lets us see relationships between multiple pairs of variables as well as distributions of single variables. Here we are using the seaborn visualization library and the PairGrid function to create a Pairs Plot with scatterplots on the upper triangle, histograms on the diagonal, and 2D kernel density plots and correlation coefficients on the lower triangle.\n",
        "\n",
        "If you don't understand this code, that's all right! Plotting in Python can be overly complex, and for anything beyond the simplest graphs, I usually find an existing implementation and adapt the code (don't repeat yourself)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7b185a4e-ac04-4ff2-b5cb-46eacf6a70b6",
        "_uuid": "9400f9d2810f4331005c9b91e040818279d1eaf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:38:01.900231Z",
          "iopub.status.busy": "2023-11-15T16:38:01.899760Z",
          "iopub.status.idle": "2023-11-15T16:40:31.217740Z",
          "shell.execute_reply": "2023-11-15T16:40:31.216815Z",
          "shell.execute_reply.started": "2023-11-15T16:38:01.900183Z"
        },
        "id": "CvmW226JpVpe",
        "outputId": "c52e7d30-1bc4-40e5-ebe4-0de34fdf1204",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Copy the data for plotting\n",
        "plot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n",
        "\n",
        "# Add in the age of the client in years\n",
        "plot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n",
        "\n",
        "# Drop na values and limit to first 100000 rows\n",
        "plot_data = plot_data.dropna().loc[:100000, :]\n",
        "\n",
        "# Function to calculate correlation coefficient between two columns\n",
        "def corr_func(x, y, **kwargs):\n",
        "    r = np.corrcoef(x, y)[0][1]\n",
        "    ax = plt.gca()\n",
        "    ax.annotate(\"r = {:.2f}\".format(r),\n",
        "                xy=(.2, .8), xycoords=ax.transAxes,\n",
        "                size = 20)\n",
        "\n",
        "# Create the pairgrid object\n",
        "grid = sns.PairGrid(data = plot_data, diag_sharey=False, hue = 'TARGET', vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n",
        "\n",
        "# Upper is a scatter plot\n",
        "grid.map_upper(plt.scatter, alpha = 0.2)\n",
        "\n",
        "# Diagonal is a histogram\n",
        "grid.map_diag(sns.kdeplot)\n",
        "\n",
        "# Bottom is density plot\n",
        "grid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n",
        "\n",
        "plt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "839f51f5-02f4-472d-9de4-aa2f760c171c",
        "_uuid": "88f9f486c74856bd87ff7699998088d9ee7fd926",
        "id": "jhzdbyvPpVpe"
      },
      "source": [
        "In this plot, the red indicates loans that were not repaid and the blue are loans that are paid. We can see the different relationships within the data. There does appear to be a moderate positive linear relationship between the `EXT_SOURCE_1` and the `DAYS_BIRTH` (or equivalently `YEARS_BIRTH`), indicating that this feature may take into account the age of the client."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bd49d18b-e35f-4122-a005-dd06d8f2f7ca",
        "_uuid": "d5506d0483af10dbf71e8ed11c99b2d5253680fb",
        "id": "H4dkToCUpVpe"
      },
      "source": [
        "# Feature Engineering\n",
        "\n",
        "* Polynomial features\n",
        "* Domain knowledge features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "464705a1-7ecf-47ba-a1fe-9f870102eb85",
        "_uuid": "70322dd11709dcaaf879a56103fde8fc787b7d4c",
        "id": "0B0C2QtfpVpe"
      },
      "source": [
        "## Polynomial Features\n",
        "\n",
        "One simple feature construction method is called [polynomial features](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html). In this method, we make features that are powers of existing features as well as interaction terms between existing features. For example, we can create variables `EXT_SOURCE_1^2` and `EXT_SOURCE_2^2` and also variables such as `EXT_SOURCE_1` x `EXT_SOURCE_2`, `EXT_SOURCE_1` x `EXT_SOURCE_2^2`, `EXT_SOURCE_1^2` x   `EXT_SOURCE_2^2`, and so on. These features that are a combination of multiple individual variables are called [interaction terms](https://en.wikipedia.org/wiki/Interaction_(statistics) because they  capture the interactions between variables. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e5b0efd9-67ac-4aa0-91e9-2141a87a6a8a",
        "_uuid": "a63d53dcac14c4ac2e31ea9c5e16b5d161c2415b",
        "execution": {
          "iopub.execute_input": "2023-11-15T16:40:31.219184Z",
          "iopub.status.busy": "2023-11-15T16:40:31.218923Z",
          "iopub.status.idle": "2023-11-15T16:40:31.484201Z",
          "shell.execute_reply": "2023-11-15T16:40:31.483255Z",
          "shell.execute_reply.started": "2023-11-15T16:40:31.219139Z"
        },
        "id": "Q78C0rmDpVpe",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Make a new dataframe for polynomial features\n",
        "poly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\n",
        "poly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
        "\n",
        "# imputer for handling missing values\n",
        "from sklearn.impute import SimpleImputer as Imputer\n",
        "imputer = Imputer(strategy = 'median')\n",
        "\n",
        "poly_target = poly_features['TARGET']\n",
        "\n",
        "poly_features = poly_features.drop(columns = ['TARGET'])\n",
        "\n",
        "# Need to impute missing values\n",
        "poly_features = imputer.fit_transform(poly_features)\n",
        "poly_features_test = imputer.transform(poly_features_test)\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Create the polynomial object with specified degree\n",
        "poly_transformer = PolynomialFeatures(degree = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2be7c1ab-d1e5-40f2-b8e7-e2b2ce1e2f9a",
        "_uuid": "72c5ecaae9c6ff038d16cbd9208f1abb69912631",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:40:31.485981Z",
          "iopub.status.busy": "2023-11-15T16:40:31.485665Z",
          "iopub.status.idle": "2023-11-15T16:40:31.875395Z",
          "shell.execute_reply": "2023-11-15T16:40:31.874410Z",
          "shell.execute_reply.started": "2023-11-15T16:40:31.485928Z"
        },
        "id": "v4Ez5jrrpVpe",
        "outputId": "fc793be6-ae1a-4a6b-dfbf-539c78565036",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Train the polynomial features\n",
        "poly_transformer.fit(poly_features)\n",
        "\n",
        "# Transform the features\n",
        "poly_features = poly_transformer.transform(poly_features)\n",
        "poly_features_test = poly_transformer.transform(poly_features_test)\n",
        "print('Polynomial Features shape: ', poly_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a7833b1e-714c-4988-8cbf-757d01290d8f",
        "_uuid": "4d837e47bada5411ffce06266605f043c6ffe19e",
        "id": "KprEBwDApVpe"
      },
      "source": [
        "This creates a considerable number of new features. To get the names we have to use the polynomial features `get_feature_names` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7465d1e6-d360-4029-afa7-67cb34f60249",
        "_uuid": "121f98d2ec9c81c5dabb911dc68562d0b2b6d737",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:40:31.877337Z",
          "iopub.status.busy": "2023-11-15T16:40:31.877039Z",
          "iopub.status.idle": "2023-11-15T16:40:31.885477Z",
          "shell.execute_reply": "2023-11-15T16:40:31.884203Z",
          "shell.execute_reply.started": "2023-11-15T16:40:31.877286Z"
        },
        "id": "dLF00EDkpVpe",
        "outputId": "2471bcad-76b0-4ca3-f974-01b7a5791e92",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "poly_transformer.get_feature_names_out(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "95725a63-f8f2-4680-8f7a-4252f04e7f7f",
        "_uuid": "e712923de757457bb87a35ecaccd27007b351e6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:40:31.887694Z",
          "iopub.status.busy": "2023-11-15T16:40:31.887186Z",
          "iopub.status.idle": "2023-11-15T16:40:32.997249Z",
          "shell.execute_reply": "2023-11-15T16:40:32.996245Z",
          "shell.execute_reply.started": "2023-11-15T16:40:31.887590Z"
        },
        "id": "rhFMvjIJpVpf",
        "outputId": "d8cb65d0-a130-4a22-bd27-a3e1da32297e",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Create a dataframe of the features\n",
        "poly_features = pd.DataFrame(poly_features,\n",
        "                             columns = poly_transformer.get_feature_names_out(['EXT_SOURCE_1', 'EXT_SOURCE_2',\n",
        "                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n",
        "\n",
        "# Add in the target\n",
        "poly_features['TARGET'] = poly_target\n",
        "\n",
        "# Find the correlations with the target\n",
        "poly_corrs = poly_features.corr()['TARGET'].sort_values()\n",
        "\n",
        "# Display most negative and most positive\n",
        "print(poly_corrs.head(10))\n",
        "print(poly_corrs.tail(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "971de432-c65e-4c9a-a3c0-c923ed27ddcb",
        "_uuid": "082ac97a068afed6758ed191acf5ab485e39230c",
        "id": "sNzU5x9PpVpf"
      },
      "source": [
        "Several of the new variables have a greater (in terms of absolute magnitude) correlation with the target than the original features. When we build machine learning models, we can try with and without these features to determine if they actually help the model learn.\n",
        "\n",
        "We will add these features to a copy of the training and testing data and then evaluate models with and without the features. Many times in machine learning, the only way to know if an approach will work is to try it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "ed758ed436a86f92a8ee574999aa91089242ca7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:40:32.999197Z",
          "iopub.status.busy": "2023-11-15T16:40:32.998639Z",
          "iopub.status.idle": "2023-11-15T16:40:36.703876Z",
          "shell.execute_reply": "2023-11-15T16:40:36.702890Z",
          "shell.execute_reply.started": "2023-11-15T16:40:32.999136Z"
        },
        "id": "WYjeg6hLpVpf",
        "outputId": "54cd568e-8170-4714-a4b4-38b0e8dc682c",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Put test features into dataframe\n",
        "poly_features_test = pd.DataFrame(poly_features_test,\n",
        "                                  columns = poly_transformer.get_feature_names_out(['EXT_SOURCE_1', 'EXT_SOURCE_2',\n",
        "                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n",
        "\n",
        "# Merge polynomial features into training dataframe\n",
        "poly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\n",
        "app_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n",
        "\n",
        "# Merge polnomial features into testing dataframe\n",
        "poly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\n",
        "app_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n",
        "\n",
        "# Align the dataframes\n",
        "app_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n",
        "\n",
        "# Print out the new shapes\n",
        "print('Training data with polynomial features shape: ', app_train_poly.shape)\n",
        "print('Testing data with polynomial features shape:  ', app_test_poly.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "9b27fad1522263c32b57a8127c84ad0e08ff9d8f",
        "id": "D9Rdfu0PpVpf"
      },
      "source": [
        "## Domain Knowledge Features\n",
        "\n",
        "\n",
        "* `CREDIT_INCOME_PERCENT`: the percentage of the credit amount relative to a client's income\n",
        "* `ANNUITY_INCOME_PERCENT`: the percentage of the loan annuity relative to a client's income\n",
        "* `CREDIT_TERM`:  the length of the payment in months (since the annuity is the monthly amount due\n",
        "* `DAYS_EMPLOYED_PERCENT`: the percentage of the days employed relative to the client's age\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "c8d4b165b45da6c3120911de18e9348d8726c70c",
        "execution": {
          "iopub.execute_input": "2023-11-15T16:40:36.705560Z",
          "iopub.status.busy": "2023-11-15T16:40:36.705285Z",
          "iopub.status.idle": "2023-11-15T16:40:37.008288Z",
          "shell.execute_reply": "2023-11-15T16:40:37.007165Z",
          "shell.execute_reply.started": "2023-11-15T16:40:36.705499Z"
        },
        "id": "P7ZAFl78pVpf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "app_train_domain = app_train.copy()\n",
        "app_test_domain = app_test.copy()\n",
        "\n",
        "app_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\n",
        "app_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']\n",
        "app_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\n",
        "app_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "d017103871bd4935a8c29599d6be33e0e74b2f83",
        "execution": {
          "iopub.execute_input": "2023-11-15T16:40:37.009980Z",
          "iopub.status.busy": "2023-11-15T16:40:37.009626Z",
          "iopub.status.idle": "2023-11-15T16:40:37.021771Z",
          "shell.execute_reply": "2023-11-15T16:40:37.020807Z",
          "shell.execute_reply.started": "2023-11-15T16:40:37.009918Z"
        },
        "id": "-eECV2NPpVpf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\n",
        "app_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\n",
        "app_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\n",
        "app_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "7e917d654c05bd0ca3251d4f51c8176d82fe613f",
        "id": "_Dz6nNWIpVpg"
      },
      "source": [
        "#### Visualize New Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "e9c10d7f55b4c636335f815762b93598fe4acb0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:40:37.024163Z",
          "iopub.status.busy": "2023-11-15T16:40:37.023838Z",
          "iopub.status.idle": "2023-11-15T16:40:38.239215Z",
          "shell.execute_reply": "2023-11-15T16:40:38.238011Z",
          "shell.execute_reply.started": "2023-11-15T16:40:37.024109Z"
        },
        "id": "-5FJpAeKpVpg",
        "outputId": "623b17c2-3a1b-43ab-ca6b-09514136b00f",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (12, 20))\n",
        "# iterate through the new features\n",
        "for i, feature in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']):\n",
        "\n",
        "    # create a new subplot for each source\n",
        "    plt.subplot(4, 1, i + 1)\n",
        "    # plot repaid loans\n",
        "    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 0, feature], label = 'target == 0')\n",
        "    # plot loans that were not repaid\n",
        "    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 1, feature], label = 'target == 1')\n",
        "\n",
        "    # Label the plots\n",
        "    plt.title('Distribution of %s by Target Value' % feature)\n",
        "    plt.xlabel('%s' % feature); plt.ylabel('Density');\n",
        "\n",
        "plt.tight_layout(h_pad = 2.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "e27d400f3ec5447cfe5e908952351f271d521784",
        "id": "ekowg2llpVpg"
      },
      "source": [
        "It's hard to say ahead of time if these new features will be useful. The only way to tell for sure is to try them out!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ebb64e63-6222-4509-a43c-302c6435ce09",
        "_uuid": "8bf057e523b2d99833f6dc9d95fe6141fb4e325a",
        "id": "Ple_M21WpVpg"
      },
      "source": [
        "# Baseline\n",
        "\n",
        "## Logistic Regression Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "60ef8744-ca3a-4810-8439-2835fbfc1833",
        "_uuid": "784ae2f91cf7792702595a9973ba773b2acdec00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:40:38.241110Z",
          "iopub.status.busy": "2023-11-15T16:40:38.240778Z",
          "iopub.status.idle": "2023-11-15T16:41:07.470630Z",
          "shell.execute_reply": "2023-11-15T16:41:07.469830Z",
          "shell.execute_reply.started": "2023-11-15T16:40:38.241053Z"
        },
        "id": "RKViyKX5pVpg",
        "outputId": "adb41d5a-7440-449e-ef78-ca06a7d93784",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer as Imputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Drop the target from the training data\n",
        "if 'TARGET' in app_train:\n",
        "    train = app_train.drop(columns = ['TARGET'])\n",
        "else:\n",
        "    train = app_train.copy()\n",
        "\n",
        "# Feature names\n",
        "features = list(train.columns)\n",
        "\n",
        "# Copy of the testing data\n",
        "test = app_test.copy()\n",
        "\n",
        "# Median imputation of missing values\n",
        "imputer = Imputer(strategy = 'median')\n",
        "\n",
        "# Scale each feature to 0-1\n",
        "scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "\n",
        "# Fit on the training data\n",
        "imputer.fit(train)\n",
        "\n",
        "# Transform both training and testing data\n",
        "train = imputer.transform(train)\n",
        "test = imputer.transform(app_test)\n",
        "\n",
        "# Repeat with the scaler\n",
        "scaler.fit(train)\n",
        "train = scaler.transform(train)\n",
        "test = scaler.transform(test)\n",
        "\n",
        "print('Training data shape: ', train.shape)\n",
        "print('Testing data shape: ', test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6462ff85-e3b6-4a5f-b95c-9416841413b1",
        "_uuid": "9e8aba9401e8367f9902d710ba49e820294870e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:41:07.472042Z",
          "iopub.status.busy": "2023-11-15T16:41:07.471792Z",
          "iopub.status.idle": "2023-11-15T16:41:12.127500Z",
          "shell.execute_reply": "2023-11-15T16:41:12.126539Z",
          "shell.execute_reply.started": "2023-11-15T16:41:07.471998Z"
        },
        "id": "mZCIV6pjpVpg",
        "outputId": "059a8968-c7d2-4026-b23a-257bd6db5818",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Make the model with the specified regularization parameter\n",
        "log_reg = LogisticRegression(C = 0.0001)\n",
        "\n",
        "# Train on the training data\n",
        "log_reg.fit(train, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "80c77c89-3fa9-4311-b441-412a4fbb1480",
        "_uuid": "2138782ddbfc9a803dc99a938460fc27d15972a9",
        "execution": {
          "iopub.execute_input": "2023-11-15T16:41:12.129593Z",
          "iopub.status.busy": "2023-11-15T16:41:12.129089Z",
          "iopub.status.idle": "2023-11-15T16:41:12.153154Z",
          "shell.execute_reply": "2023-11-15T16:41:12.151976Z",
          "shell.execute_reply.started": "2023-11-15T16:41:12.129523Z"
        },
        "id": "N9-hoORLpVpg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "# Make sure to select the second column only\n",
        "log_reg_pred = log_reg.predict_proba(test)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "09a3d281e4c7ee6820f402e32f31775851113089",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:41:12.155519Z",
          "iopub.status.busy": "2023-11-15T16:41:12.155094Z",
          "iopub.status.idle": "2023-11-15T16:41:12.333248Z",
          "shell.execute_reply": "2023-11-15T16:41:12.332272Z",
          "shell.execute_reply.started": "2023-11-15T16:41:12.155450Z"
        },
        "id": "CQKG6Fb5pVpg",
        "outputId": "4e320f41-c685-4e76-f94c-81ce0f2df7d5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Submission dataframe\n",
        "submit = app_test[['SK_ID_CURR']]\n",
        "submit['TARGET'] = log_reg_pred\n",
        "\n",
        "submit.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "77204f15-c3b9-4c67-8d93-173fa3afceaa",
        "_uuid": "fcaf338e52d8f42f119b31d437b516e336e787ec",
        "execution": {
          "iopub.execute_input": "2023-11-15T16:41:12.335602Z",
          "iopub.status.busy": "2023-11-15T16:41:12.334920Z",
          "iopub.status.idle": "2023-11-15T16:41:12.721835Z",
          "shell.execute_reply": "2023-11-15T16:41:12.720825Z",
          "shell.execute_reply.started": "2023-11-15T16:41:12.335530Z"
        },
        "id": "plT-CFsCpVph",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Save the submission to a csv file\n",
        "submit.to_csv('/Users/singh.aditya2/Downloads/home-credit-default-risk/log_baseline.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "462ea34f-3f66-490a-a61f-24a991271f69",
        "_uuid": "92687ac866441f6ee2919aa5e5c935490c172afc",
        "id": "hAXNX_BEpVph"
      },
      "source": [
        "## Improved Model: Random Forest\n",
        " we can update the algorithm. using a Random Forest on the same training data to see how that affects performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6643479e-7980-431c-a6a2-9087acdb0f42",
        "_uuid": "cf05e2318904b8f3575ae233c185cd995fd07643",
        "execution": {
          "iopub.execute_input": "2023-11-15T16:41:12.724107Z",
          "iopub.status.busy": "2023-11-15T16:41:12.723543Z",
          "iopub.status.idle": "2023-11-15T16:41:12.771764Z",
          "shell.execute_reply": "2023-11-15T16:41:12.770960Z",
          "shell.execute_reply.started": "2023-11-15T16:41:12.723890Z"
        },
        "id": "AfnUBqKdpVph",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Make the random forest classifier\n",
        "random_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "020f0856-8f24-4b22-bca5-aac7f137f032",
        "_uuid": "52258a9b89b3069bc1d82829107e8e7c1ef05fd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:41:12.773473Z",
          "iopub.status.busy": "2023-11-15T16:41:12.773192Z",
          "iopub.status.idle": "2023-11-15T16:42:49.739600Z",
          "shell.execute_reply": "2023-11-15T16:42:49.738625Z",
          "shell.execute_reply.started": "2023-11-15T16:41:12.773426Z"
        },
        "id": "Cl8oh28xpVph",
        "outputId": "0098aaf3-ceb4-41ec-bda7-3148d8046d1a",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Train on the training data\n",
        "random_forest.fit(train, train_labels)\n",
        "\n",
        "# Extract feature importances\n",
        "feature_importance_values = random_forest.feature_importances_\n",
        "feature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = random_forest.predict_proba(test)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "25145966-669e-426d-89a3-98e30b861057",
        "_uuid": "1da4b02502388d2b8a2bc5376027c5bef50272f3",
        "execution": {
          "iopub.execute_input": "2023-11-15T16:42:49.741826Z",
          "iopub.status.busy": "2023-11-15T16:42:49.741412Z",
          "iopub.status.idle": "2023-11-15T16:42:49.997342Z",
          "shell.execute_reply": "2023-11-15T16:42:49.996640Z",
          "shell.execute_reply.started": "2023-11-15T16:42:49.741750Z"
        },
        "id": "v2p86ULlpVph",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Make a submission dataframe\n",
        "submit = app_test[['SK_ID_CURR']]\n",
        "submit['TARGET'] = predictions\n",
        "\n",
        "# Save the submission dataframe\n",
        "submit.to_csv('/Users/singh.aditya2/Downloads/home-credit-default-risk/random_forest_baseline.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "43d979aed7cdfd6d7bd6a995b5756a384bd2b7dc",
        "id": "uB8iPHojpVpi"
      },
      "source": [
        "### Make Predictions using Engineered Features\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "d9d49008fb73b8d15c797850c64d5e6f81375163",
        "execution": {
          "iopub.execute_input": "2023-11-15T16:42:49.999022Z",
          "iopub.status.busy": "2023-11-15T16:42:49.998690Z",
          "iopub.status.idle": "2023-11-15T16:43:22.124572Z",
          "shell.execute_reply": "2023-11-15T16:43:22.123776Z",
          "shell.execute_reply.started": "2023-11-15T16:42:49.998951Z"
        },
        "id": "cs8jv1IYpVpi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "poly_features_names = list(app_train_poly.columns)\n",
        "\n",
        "# Impute the polynomial features\n",
        "imputer = Imputer(strategy = 'median')\n",
        "\n",
        "poly_features = imputer.fit_transform(app_train_poly)\n",
        "poly_features_test = imputer.transform(app_test_poly)\n",
        "\n",
        "# Scale the polynomial features\n",
        "scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "\n",
        "poly_features = scaler.fit_transform(poly_features)\n",
        "poly_features_test = scaler.transform(poly_features_test)\n",
        "\n",
        "random_forest_poly = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "a7d3f3b6cdf8231832c56224c8a694056e456593",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:43:22.127330Z",
          "iopub.status.busy": "2023-11-15T16:43:22.126819Z",
          "iopub.status.idle": "2023-11-15T16:45:46.284716Z",
          "shell.execute_reply": "2023-11-15T16:45:46.283888Z",
          "shell.execute_reply.started": "2023-11-15T16:43:22.127146Z"
        },
        "id": "TNdPcnA6pVpi",
        "outputId": "6bded70f-536a-4390-af6f-362ecad92c6c",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Train on the training data\n",
        "random_forest_poly.fit(poly_features, train_labels)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = random_forest_poly.predict_proba(poly_features_test)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "cd923eed057b6d61354db27473d9a36f1411dd5c",
        "execution": {
          "iopub.execute_input": "2023-11-15T16:45:46.286225Z",
          "iopub.status.busy": "2023-11-15T16:45:46.285843Z",
          "iopub.status.idle": "2023-11-15T16:45:46.554089Z",
          "shell.execute_reply": "2023-11-15T16:45:46.552999Z",
          "shell.execute_reply.started": "2023-11-15T16:45:46.286182Z"
        },
        "id": "QOP3RIxQpVpi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Make a submission dataframe\n",
        "submit = app_test[['SK_ID_CURR']]\n",
        "submit['TARGET'] = predictions\n",
        "\n",
        "# Save the submission dataframe\n",
        "submit.to_csv('random_forest_baseline_engineered.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "ec50627c874a9d78d6789e01a47e829c820f9615",
        "id": "C_rQr2WjpVpi"
      },
      "source": [
        "This model scored 0.678 \n",
        "#### Testing Domain Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "04b93e7d3629c1a5ba27a6eed037900862dc039d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:45:46.556943Z",
          "iopub.status.busy": "2023-11-15T16:45:46.556561Z",
          "iopub.status.idle": "2023-11-15T16:47:58.647395Z",
          "shell.execute_reply": "2023-11-15T16:47:58.646284Z",
          "shell.execute_reply.started": "2023-11-15T16:45:46.556880Z"
        },
        "id": "5cwUeJeJpVpj",
        "outputId": "314cb10f-56c2-4067-e46f-3287f59fe2fd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "app_train_domain = app_train_domain.drop(columns = 'TARGET')\n",
        "\n",
        "domain_features_names = list(app_train_domain.columns)\n",
        "\n",
        "# Impute the domainnomial features\n",
        "imputer = Imputer(strategy = 'median')\n",
        "\n",
        "domain_features = imputer.fit_transform(app_train_domain)\n",
        "domain_features_test = imputer.transform(app_test_domain)\n",
        "\n",
        "# Scale the domainnomial features\n",
        "scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "\n",
        "domain_features = scaler.fit_transform(domain_features)\n",
        "domain_features_test = scaler.transform(domain_features_test)\n",
        "\n",
        "random_forest_domain = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n",
        "\n",
        "# Train on the training data\n",
        "random_forest_domain.fit(domain_features, train_labels)\n",
        "\n",
        "# Extract feature importances\n",
        "feature_importance_values_domain = random_forest_domain.feature_importances_\n",
        "feature_importances_domain = pd.DataFrame({'feature': domain_features_names, 'importance': feature_importance_values_domain})\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = random_forest_domain.predict_proba(domain_features_test)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "27598fb499df4c3282be63356422e4a6f6d6dd17",
        "execution": {
          "iopub.execute_input": "2023-11-15T16:47:58.649531Z",
          "iopub.status.busy": "2023-11-15T16:47:58.649140Z",
          "iopub.status.idle": "2023-11-15T16:47:58.916050Z",
          "shell.execute_reply": "2023-11-15T16:47:58.914897Z",
          "shell.execute_reply.started": "2023-11-15T16:47:58.649453Z"
        },
        "id": "peWKr-YlpVpj",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Make a submission dataframe\n",
        "submit = app_test[['SK_ID_CURR']]\n",
        "submit['TARGET'] = predictions\n",
        "\n",
        "# Save the submission dataframe\n",
        "submit.to_csv('/Users/singh.aditya2/Downloads/home-credit-default-risk/random_forest_baseline_domain.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "240fb8ba2b5fe73da4d021543fd64baa104fb418",
        "id": "DaSVIyIcpVpj"
      },
      "source": [
        "This scores 0.679 when submitted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b742ed91-9dd6-4a7b-af5e-1d6e7128beb2",
        "_uuid": "b1805834b4d4eae38db4f68502aade956fc1e10f",
        "id": "yY9Q_-t3pVpj"
      },
      "source": [
        "## Model Interpretation: Feature Importances\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a90e9368-5f7d-4179-a5cc-1025f32c6a81",
        "_uuid": "b912337a5f35f495398d8ae8b8576ceb7062fe50",
        "execution": {
          "iopub.execute_input": "2023-11-15T16:47:58.919168Z",
          "iopub.status.busy": "2023-11-15T16:47:58.918802Z",
          "iopub.status.idle": "2023-11-15T16:47:58.927819Z",
          "shell.execute_reply": "2023-11-15T16:47:58.926679Z",
          "shell.execute_reply.started": "2023-11-15T16:47:58.919109Z"
        },
        "id": "YPOQOV2YpVpj",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def plot_feature_importances(df):\n",
        "    \"\"\"\n",
        "    Plot importances returned by a model. This can work with any measure of\n",
        "    feature importance provided that higher importance is better.\n",
        "\n",
        "    Args:\n",
        "        df (dataframe): feature importances. Must have the features in a column\n",
        "        called `features` and the importances in a column called `importance\n",
        "\n",
        "    Returns:\n",
        "        shows a plot of the 15 most importance features\n",
        "\n",
        "        df (dataframe): feature importances sorted by importance (highest to lowest)\n",
        "        with a column for normalized importance\n",
        "        \"\"\"\n",
        "\n",
        "    # Sort features according to importance\n",
        "    df = df.sort_values('importance', ascending = False).reset_index()\n",
        "\n",
        "    # Normalize the feature importances to add up to one\n",
        "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
        "\n",
        "    # Make a horizontal bar chart of feature importances\n",
        "    plt.figure(figsize = (10, 6))\n",
        "    ax = plt.subplot()\n",
        "\n",
        "    # Need to reverse the index to plot most important on top\n",
        "    ax.barh(list(reversed(list(df.index[:15]))),\n",
        "            df['importance_normalized'].head(15),\n",
        "            align = 'center', edgecolor = 'k')\n",
        "\n",
        "    # Set the yticks and labels\n",
        "    ax.set_yticks(list(reversed(list(df.index[:15]))))\n",
        "    ax.set_yticklabels(df['feature'].head(15))\n",
        "\n",
        "    # Plot labeling\n",
        "    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n",
        "    plt.show()\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1084ad42-bc44-438b-b2fd-5fd7a1c1b363",
        "_uuid": "37309c4a94b248ad85fa7a0825f01830a818ba92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:47:58.929887Z",
          "iopub.status.busy": "2023-11-15T16:47:58.929434Z",
          "iopub.status.idle": "2023-11-15T16:47:59.249906Z",
          "shell.execute_reply": "2023-11-15T16:47:59.249093Z",
          "shell.execute_reply.started": "2023-11-15T16:47:58.929795Z"
        },
        "id": "0QaFeyjVpVpj",
        "outputId": "c30f18c4-3b53-4b22-bc75-589220dcd836",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Show the feature importances for the default features\n",
        "feature_importances_sorted = plot_feature_importances(feature_importances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "516e4b2eedeec2ff441f1ff034fbe4a73374bba2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:47:59.251727Z",
          "iopub.status.busy": "2023-11-15T16:47:59.251200Z",
          "iopub.status.idle": "2023-11-15T16:47:59.563531Z",
          "shell.execute_reply": "2023-11-15T16:47:59.562558Z",
          "shell.execute_reply.started": "2023-11-15T16:47:59.251643Z"
        },
        "id": "FdFEp0uXpVpj",
        "outputId": "987e1e6b-e28b-4c04-eae9-50f7188b0b95",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "feature_importances_domain_sorted = plot_feature_importances(feature_importances_domain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d12452cd-347e-4269-b3d4-f5f0589f4c5c",
        "_uuid": "a8bc307f9be27bfabbc3891deddbd94293ca03fa",
        "id": "I12GE_izpVpk"
      },
      "source": [
        "# Light Gradient Boosting Machine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "60208a3f-947f-42d9-8f46-2159afd2eb7d",
        "_uuid": "2719663ed461422fce26b5dd55a31ab9718df47a",
        "execution": {
          "iopub.execute_input": "2023-11-15T16:47:59.565830Z",
          "iopub.status.busy": "2023-11-15T16:47:59.565460Z",
          "iopub.status.idle": "2023-11-15T16:47:59.637208Z",
          "shell.execute_reply": "2023-11-15T16:47:59.636380Z",
          "shell.execute_reply.started": "2023-11-15T16:47:59.565767Z"
        },
        "id": "nAAyCPJppVpk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import lightgbm as lgb\n",
        "import gc\n",
        "\n",
        "def model(features, test_features, encoding = 'ohe', n_folds = 5):\n",
        "\n",
        "    \"\"\"Train and test a light gradient boosting model using\n",
        "    cross validation.\n",
        "\n",
        "    Parameters\n",
        "    --------\n",
        "        features (pd.DataFrame):\n",
        "            dataframe of training features to use\n",
        "            for training a model. Must include the TARGET column.\n",
        "        test_features (pd.DataFrame):\n",
        "            dataframe of testing features to use\n",
        "            for making predictions with the model.\n",
        "        encoding (str, default = 'ohe'):\n",
        "            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n",
        "            n_folds (int, default = 5): number of folds to use for cross validation\n",
        "\n",
        "    Return\n",
        "    --------\n",
        "        submission (pd.DataFrame):\n",
        "            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n",
        "            predicted by the model.\n",
        "        feature_importances (pd.DataFrame):\n",
        "            dataframe with the feature importances from the model.\n",
        "        valid_metrics (pd.DataFrame):\n",
        "            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract the ids\n",
        "    train_ids = features['SK_ID_CURR']\n",
        "    test_ids = test_features['SK_ID_CURR']\n",
        "\n",
        "    # Extract the labels for training\n",
        "    labels = features['TARGET']\n",
        "\n",
        "    # Remove the ids and target\n",
        "    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
        "    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n",
        "\n",
        "\n",
        "    # One Hot Encoding\n",
        "    if encoding == 'ohe':\n",
        "        features = pd.get_dummies(features)\n",
        "        test_features = pd.get_dummies(test_features)\n",
        "\n",
        "        # Align the dataframes by the columns\n",
        "        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n",
        "\n",
        "        # No categorical indices to record\n",
        "        cat_indices = 'auto'\n",
        "\n",
        "    # Integer label encoding\n",
        "    elif encoding == 'le':\n",
        "\n",
        "        # Create a label encoder\n",
        "        label_encoder = LabelEncoder()\n",
        "\n",
        "        # List for storing categorical indices\n",
        "        cat_indices = []\n",
        "\n",
        "        # Iterate through each column\n",
        "        for i, col in enumerate(features):\n",
        "            if features[col].dtype == 'object':\n",
        "                # Map the categorical features to integers\n",
        "                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n",
        "                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n",
        "\n",
        "                # Record the categorical indices\n",
        "                cat_indices.append(i)\n",
        "\n",
        "    # Catch error if label encoding scheme is not valid\n",
        "    else:\n",
        "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
        "\n",
        "    print('Training Data Shape: ', features.shape)\n",
        "    print('Testing Data Shape: ', test_features.shape)\n",
        "\n",
        "    # Extract feature names\n",
        "    feature_names = list(features.columns)\n",
        "\n",
        "    # Convert to np arrays\n",
        "    features = np.array(features)\n",
        "    test_features = np.array(test_features)\n",
        "\n",
        "    # Create the kfold object\n",
        "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
        "\n",
        "    # Empty array for feature importances\n",
        "    feature_importance_values = np.zeros(len(feature_names))\n",
        "\n",
        "    # Empty array for test predictions\n",
        "    test_predictions = np.zeros(test_features.shape[0])\n",
        "\n",
        "    # Empty array for out of fold validation predictions\n",
        "    out_of_fold = np.zeros(features.shape[0])\n",
        "\n",
        "    # Lists for recording validation and training scores\n",
        "    valid_scores = []\n",
        "    train_scores = []\n",
        "\n",
        "    # Iterate through each fold\n",
        "    for train_indices, valid_indices in k_fold.split(features):\n",
        "\n",
        "        # Training data for the fold\n",
        "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
        "        # Validation data for the fold\n",
        "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
        "\n",
        "        # Create the model\n",
        "        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary',\n",
        "                                   class_weight = 'balanced', learning_rate = 0.05,\n",
        "                                   reg_alpha = 0.1, reg_lambda = 0.1,\n",
        "                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
        "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
        "                  eval_names = ['valid', 'train'], categorical_feature = cat_indices)\n",
        "\n",
        "        # Record the best iteration\n",
        "        best_iteration = model.best_iteration_\n",
        "\n",
        "        # Record the feature importances\n",
        "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
        "\n",
        "        # Make predictions\n",
        "        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n",
        "\n",
        "        # Record the out of fold predictions\n",
        "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
        "\n",
        "        # Record the best score\n",
        "        valid_score = model.best_score_['valid']['auc']\n",
        "        train_score = model.best_score_['train']['auc']\n",
        "\n",
        "        valid_scores.append(valid_score)\n",
        "        train_scores.append(train_score)\n",
        "\n",
        "        # Clean up memory\n",
        "        gc.enable()\n",
        "        del model, train_features, valid_features\n",
        "        gc.collect()\n",
        "\n",
        "    # Make the submission dataframe\n",
        "    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n",
        "\n",
        "    # Make the feature importance dataframe\n",
        "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
        "\n",
        "    # Overall validation score\n",
        "    valid_auc = roc_auc_score(labels, out_of_fold)\n",
        "\n",
        "    # Add the overall scores to the metrics\n",
        "    valid_scores.append(valid_auc)\n",
        "    train_scores.append(np.mean(train_scores))\n",
        "\n",
        "    # Needed for creating dataframe of validation scores\n",
        "    fold_names = list(range(n_folds))\n",
        "    fold_names.append('overall')\n",
        "\n",
        "    # Dataframe of validation scores\n",
        "    metrics = pd.DataFrame({'fold': fold_names,\n",
        "                            'train': train_scores,\n",
        "                            'valid': valid_scores})\n",
        "\n",
        "    return submission, feature_importances, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "89e02dcbb23e47e3504ed1f61431b182e2011ba5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-15T16:47:59.638753Z",
          "iopub.status.busy": "2023-11-15T16:47:59.638287Z",
          "iopub.status.idle": "2023-11-15T16:52:51.091085Z",
          "shell.execute_reply": "2023-11-15T16:52:51.089551Z",
          "shell.execute_reply.started": "2023-11-15T16:47:59.638699Z"
        },
        "id": "hgmpn-GPpVpk",
        "outputId": "29cc1824-d937-47b3-fa2a-cb0f69dd1c6a",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "submission, fi, metrics = model(app_train, app_test)\n",
        "print('Baseline metrics')\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "ca59467edd1060e5f7587a77a31dcd7331ce90ec",
        "execution": {
          "iopub.execute_input": "2023-11-15T16:52:51.092965Z",
          "iopub.status.busy": "2023-11-15T16:52:51.092599Z",
          "iopub.status.idle": "2023-11-15T16:52:51.421121Z",
          "shell.execute_reply": "2023-11-15T16:52:51.419563Z",
          "shell.execute_reply.started": "2023-11-15T16:52:51.092893Z"
        },
        "id": "V_K1EDCVpVpk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fi_sorted = plot_feature_importances(fi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "d71f9d7b9b322824704eec9dc82e38a480d4f76c",
        "execution": {
          "iopub.execute_input": "2023-11-15T16:52:51.423399Z",
          "iopub.status.busy": "2023-11-15T16:52:51.422982Z",
          "iopub.status.idle": "2023-11-15T16:52:51.610673Z",
          "shell.execute_reply": "2023-11-15T16:52:51.609721Z",
          "shell.execute_reply.started": "2023-11-15T16:52:51.423322Z"
        },
        "id": "KUzMbLsvpVpk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "submission.to_csv('/Users/singh.aditya2/Downloads/home-credit-default-risk/baseline_lgb.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "2aca0b9ea31dfef1ca3221dc6424fe31e829cbbf",
        "id": "-mg2HUoTpVpk"
      },
      "source": [
        "This submission should score about 0.735 on the leaderboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "cd53d758d2838c9b99b9ae44780514d13373b717",
        "execution": {
          "iopub.execute_input": "2023-11-15T16:52:51.612566Z",
          "iopub.status.busy": "2023-11-15T16:52:51.612158Z",
          "iopub.status.idle": "2023-11-15T16:58:16.960666Z",
          "shell.execute_reply": "2023-11-15T16:58:16.959229Z",
          "shell.execute_reply.started": "2023-11-15T16:52:51.612486Z"
        },
        "id": "9IRwEycmpVpk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "app_train_domain['TARGET'] = train_labels\n",
        "\n",
        "# Test the domain knolwedge features\n",
        "submission_domain, fi_domain, metrics_domain = model(app_train_domain, app_test_domain)\n",
        "print('Baseline with domain knowledge features metrics')\n",
        "print(metrics_domain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "58a2d9b330a223733e3673b24433c41122d3b611",
        "execution": {
          "iopub.execute_input": "2023-11-15T16:58:16.962137Z",
          "iopub.status.busy": "2023-11-15T16:58:16.961858Z",
          "iopub.status.idle": "2023-11-15T16:58:17.275554Z",
          "shell.execute_reply": "2023-11-15T16:58:17.274680Z",
          "shell.execute_reply.started": "2023-11-15T16:58:16.962092Z"
        },
        "id": "K3yWaFEYpVpk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fi_sorted = plot_feature_importances(fi_domain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "7dfc9123c7e231826be54a1c022e373a1ee68f51",
        "execution": {
          "iopub.execute_input": "2023-11-15T16:58:17.277100Z",
          "iopub.status.busy": "2023-11-15T16:58:17.276845Z",
          "iopub.status.idle": "2023-11-15T16:58:17.461248Z",
          "shell.execute_reply": "2023-11-15T16:58:17.460208Z",
          "shell.execute_reply.started": "2023-11-15T16:58:17.277056Z"
        },
        "id": "_u9XTW71pVpk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "submission_domain.to_csv('/Users/singh.aditya2/Downloads/home-credit-default-risk/baseline_lgb_domain_features.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vvb1RHQBpVpl"
      },
      "source": [
        "# Feature Engineering\n",
        "\n",
        "\n",
        "## Manual Feature Engineering with bureau_balance and bureau data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP1HhL2lpVpl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Read in bureau\n",
        "bureau = pd.read_csv('/Users/singh.aditya2/Downloads/home-credit-default-risk/bureau.csv')\n",
        "bureau.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:11:42.139765Z",
          "iopub.status.busy": "2023-11-15T18:11:42.139443Z",
          "iopub.status.idle": "2023-11-15T18:11:42.309587Z",
          "shell.execute_reply": "2023-11-15T18:11:42.308952Z",
          "shell.execute_reply.started": "2023-11-15T18:11:42.139703Z"
        },
        "id": "JOX2_h91pVpl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Groupby the client id (SK_ID_CURR), count the number of previous loans, and rename the column\n",
        "previous_loan_counts = bureau.groupby('SK_ID_CURR', as_index=False)['SK_ID_BUREAU'].count().rename(columns = {'SK_ID_BUREAU': 'previous_loan_counts'})\n",
        "previous_loan_counts.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:11:42.310986Z",
          "iopub.status.busy": "2023-11-15T18:11:42.310551Z",
          "iopub.status.idle": "2023-11-15T18:11:44.373088Z",
          "shell.execute_reply": "2023-11-15T18:11:44.372202Z",
          "shell.execute_reply.started": "2023-11-15T18:11:42.310940Z"
        },
        "id": "ocqKmAyLpVpl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# take the training data from the previous baseline\n",
        "train = pd.read_csv('/Users/singh.aditya2/Downloads/home-credit-default-risk/application_train.csv')\n",
        "train = train.merge(previous_loan_counts, on = 'SK_ID_CURR', how = 'left')\n",
        "\n",
        "# Fill the missing values with 0 \n",
        "train['previous_loan_counts'] = train['previous_loan_counts'].fillna(0)\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:11:49.215080Z",
          "iopub.status.busy": "2023-11-15T18:11:49.214362Z",
          "iopub.status.idle": "2023-11-15T18:11:49.221575Z",
          "shell.execute_reply": "2023-11-15T18:11:49.220824Z",
          "shell.execute_reply.started": "2023-11-15T18:11:49.215016Z"
        },
        "id": "0ITNejjSpVpm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def kde_target(var_name, df):\n",
        "    \n",
        "    # Calculate the correlation coefficient between the new variable and the target\n",
        "    corr = df['TARGET'].corr(df[var_name])\n",
        "    \n",
        "    # Calculate medians for repaid vs not repaid\n",
        "    avg_repaid = df.loc[df['TARGET'] == 0, var_name].median()\n",
        "    avg_not_repaid = df.loc[df['TARGET'] == 1, var_name].median()\n",
        "    \n",
        "    plt.figure(figsize = (12, 6))\n",
        "    \n",
        "    # Plot the distribution for target == 0 and target == 1\n",
        "    sns.kdeplot(df.loc[df['TARGET'] == 0, var_name], label = 'TARGET == 0')\n",
        "    sns.kdeplot(df.loc[df['TARGET'] == 1, var_name], label = 'TARGET == 1')\n",
        "    \n",
        "    # label the plot\n",
        "    plt.xlabel(var_name); plt.ylabel('Density'); plt.title('%s Distribution' % var_name)\n",
        "    plt.legend()\n",
        "    \n",
        "    # print out the correlation\n",
        "    print('The correlation between %s and the TARGET is %0.4f' % (var_name, corr))\n",
        "    # Print out average values\n",
        "    print('Median value for loan that was not repaid = %0.4f' % avg_not_repaid)\n",
        "    print('Median value for loan that was repaid =     %0.4f' % avg_repaid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kde_target('EXT_SOURCE_3', train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kde_target('previous_loan_counts', train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bureau_agg = bureau.drop(columns = ['SK_ID_BUREAU']).groupby('SK_ID_CURR', as_index = False).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\n",
        "bureau_agg.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:13:41.202168Z",
          "iopub.status.busy": "2023-11-15T18:13:41.201759Z",
          "iopub.status.idle": "2023-11-15T18:13:41.208437Z",
          "shell.execute_reply": "2023-11-15T18:13:41.207374Z",
          "shell.execute_reply.started": "2023-11-15T18:13:41.202099Z"
        },
        "id": "VNHWcAcapVpm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# List of column names\n",
        "columns = ['SK_ID_CURR']\n",
        "\n",
        "# Iterate through the variables names\n",
        "for var in bureau_agg.columns.levels[0]:\n",
        "    # Skip the id name\n",
        "    if var != 'SK_ID_CURR':\n",
        "\n",
        "        # Iterate through the stat names\n",
        "        for stat in bureau_agg.columns.levels[1][:-1]:\n",
        "            # Make a new column name for the variable and stat\n",
        "            columns.append('bureau_%s_%s' % (var, stat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:13:49.197933Z",
          "iopub.status.busy": "2023-11-15T18:13:49.197243Z",
          "iopub.status.idle": "2023-11-15T18:13:49.309425Z",
          "shell.execute_reply": "2023-11-15T18:13:49.308358Z",
          "shell.execute_reply.started": "2023-11-15T18:13:49.197532Z"
        },
        "id": "MBKWBlv6pVpm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "bureau_agg.columns = columns\n",
        "bureau_agg.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:13:57.872823Z",
          "iopub.status.busy": "2023-11-15T18:13:57.872461Z",
          "iopub.status.idle": "2023-11-15T18:14:00.855715Z",
          "shell.execute_reply": "2023-11-15T18:14:00.854734Z",
          "shell.execute_reply.started": "2023-11-15T18:13:57.872764Z"
        },
        "id": "vvJPH9LhpVpm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train = train.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:14:12.433557Z",
          "iopub.status.busy": "2023-11-15T18:14:12.433234Z",
          "iopub.status.idle": "2023-11-15T18:14:12.952050Z",
          "shell.execute_reply": "2023-11-15T18:14:12.951061Z",
          "shell.execute_reply.started": "2023-11-15T18:14:12.433508Z"
        },
        "id": "rkqohfHYpVpm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "new_corrs = []\n",
        "\n",
        "# Iterate through the columns\n",
        "for col in columns:\n",
        "    # Calculate correlation with the target\n",
        "    corr = train['TARGET'].corr(train[col])\n",
        "\n",
        "    # Append the list as a tuple\n",
        "\n",
        "    new_corrs.append((col, corr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hymC8xy4pVpm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Sort the correlations by the absolute value\n",
        "# Make sure to reverse to put the largest values at the front of list\n",
        "new_corrs = sorted(new_corrs, key = lambda x: abs(x[1]), reverse = True)\n",
        "new_corrs[:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:54:30.301214Z",
          "iopub.status.busy": "2023-11-15T18:54:30.300701Z",
          "iopub.status.idle": "2023-11-15T18:54:30.311530Z",
          "shell.execute_reply": "2023-11-15T18:54:30.310511Z",
          "shell.execute_reply.started": "2023-11-15T18:54:30.301163Z"
        },
        "id": "wl16eT_4pVpn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def agg_numeric(df, group_var, df_name):\n",
        "    \"\"\"Aggregates the numeric values in a dataframe. This can\n",
        "    be used to create features for each instance of the grouping variable.\n",
        "\n",
        "    Parameters\n",
        "    --------\n",
        "        df (dataframe):\n",
        "            the dataframe to calculate the statistics on\n",
        "        group_var (string):\n",
        "            the variable by which to group df\n",
        "        df_name (string):\n",
        "            the variable used to rename the columns\n",
        "\n",
        "    Return\n",
        "    --------\n",
        "        agg (dataframe):\n",
        "            a dataframe with the statistics aggregated for\n",
        "            all numeric columns. Each instance of the grouping variable will have\n",
        "            the statistics (mean, min, max, sum; currently supported) calculated.\n",
        "            The columns are also renamed to keep track of features created.\n",
        "\n",
        "    \"\"\"\n",
        "    # Remove id variables other than grouping variable\n",
        "    for col in df:\n",
        "        if col != group_var and 'SK_ID' in col:\n",
        "            df = df.drop(columns = col)\n",
        "\n",
        "    group_ids = df[group_var]\n",
        "    numeric_df = df.select_dtypes('number')\n",
        "    numeric_df[group_var] = group_ids\n",
        "\n",
        "    # Group by the specified variable and calculate the statistics\n",
        "    agg = numeric_df.groupby(group_var).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\n",
        "\n",
        "    # Need to create new column names\n",
        "    columns = [group_var]\n",
        "\n",
        "    # Iterate through the variables names\n",
        "    for var in agg.columns.levels[0]:\n",
        "        # Skip the grouping variable\n",
        "        if var != group_var:\n",
        "            # Iterate through the stat names\n",
        "            for stat in agg.columns.levels[1][:-1]:\n",
        "                # Make a new column name for the variable and stat\n",
        "                columns.append('%s_%s_%s' % (df_name, var, stat))\n",
        "\n",
        "    agg.columns = columns\n",
        "\n",
        "       # Remove the columns with all redundant values\n",
        "    _, idx = np.unique(agg, axis = 1, return_index=True)\n",
        "    agg = agg.iloc[:, idx]\n",
        "    return agg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:54:36.591370Z",
          "iopub.status.busy": "2023-11-15T18:54:36.591019Z",
          "iopub.status.idle": "2023-11-15T18:54:41.839018Z",
          "shell.execute_reply": "2023-11-15T18:54:41.837868Z",
          "shell.execute_reply.started": "2023-11-15T18:54:36.591316Z"
        },
        "id": "VlCd05VNpVpn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "bureau_agg_new = agg_numeric(bureau.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'bureau')\n",
        "bureau_agg_new.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:54:45.096099Z",
          "iopub.status.busy": "2023-11-15T18:54:45.095560Z",
          "iopub.status.idle": "2023-11-15T18:54:45.211465Z",
          "shell.execute_reply": "2023-11-15T18:54:45.210803Z",
          "shell.execute_reply.started": "2023-11-15T18:54:45.096047Z"
        },
        "id": "nHISFd8bpVpn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "bureau_agg.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:54:59.385234Z",
          "iopub.status.busy": "2023-11-15T18:54:59.384831Z",
          "iopub.status.idle": "2023-11-15T18:54:59.391476Z",
          "shell.execute_reply": "2023-11-15T18:54:59.390605Z",
          "shell.execute_reply.started": "2023-11-15T18:54:59.385162Z"
        },
        "id": "wZrSrGMfpVpn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def target_corrs(df):\n",
        "\n",
        "    # List of correlations\n",
        "    corrs = []\n",
        "\n",
        "    # Iterate through the columns\n",
        "    for col in df.columns:\n",
        "        print(col)\n",
        "        # Skip the target column\n",
        "        if col != 'TARGET':\n",
        "            # Calculate correlation with the target\n",
        "            corr = df['TARGET'].corr(df[col])\n",
        "\n",
        "            # Append the list as a tuple\n",
        "            corrs.append((col, corr))\n",
        "\n",
        "    # Sort by absolute magnitude of correlations\n",
        "    corrs = sorted(corrs, key = lambda x: abs(x[1]), reverse = True)\n",
        "\n",
        "    return corrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:55:25.505800Z",
          "iopub.status.busy": "2023-11-15T18:55:25.505214Z",
          "iopub.status.idle": "2023-11-15T18:55:26.085495Z",
          "shell.execute_reply": "2023-11-15T18:55:26.084503Z",
          "shell.execute_reply.started": "2023-11-15T18:55:25.505724Z"
        },
        "id": "PljAV7ywpVpn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Categorical variables\n",
        "\n",
        "categorical = pd.get_dummies(bureau.select_dtypes('object'))\n",
        "categorical['SK_ID_CURR'] = bureau['SK_ID_CURR']\n",
        "categorical.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:55:35.436797Z",
          "iopub.status.busy": "2023-11-15T18:55:35.436428Z",
          "iopub.status.idle": "2023-11-15T18:55:38.565988Z",
          "shell.execute_reply": "2023-11-15T18:55:38.564817Z",
          "shell.execute_reply.started": "2023-11-15T18:55:35.436733Z"
        },
        "id": "kuWwpiJupVpn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "categorical_grouped = categorical.groupby('SK_ID_CURR').agg(['sum', 'mean'])\n",
        "categorical_grouped.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:55:55.486089Z",
          "iopub.status.busy": "2023-11-15T18:55:55.485758Z",
          "iopub.status.idle": "2023-11-15T18:55:55.493555Z",
          "shell.execute_reply": "2023-11-15T18:55:55.492424Z",
          "shell.execute_reply.started": "2023-11-15T18:55:55.486039Z"
        },
        "id": "F5_Ij8kdpVpn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "categorical_grouped.columns.levels[0][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:56:06.540550Z",
          "iopub.status.busy": "2023-11-15T18:56:06.540183Z",
          "iopub.status.idle": "2023-11-15T18:56:06.547443Z",
          "shell.execute_reply": "2023-11-15T18:56:06.546487Z",
          "shell.execute_reply.started": "2023-11-15T18:56:06.540482Z"
        },
        "id": "NcPHTdx8pVpn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "categorical_grouped.columns.levels[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:56:14.941141Z",
          "iopub.status.busy": "2023-11-15T18:56:14.940765Z",
          "iopub.status.idle": "2023-11-15T18:56:15.041058Z",
          "shell.execute_reply": "2023-11-15T18:56:15.039870Z",
          "shell.execute_reply.started": "2023-11-15T18:56:14.941051Z"
        },
        "id": "SXX8jtGSpVpn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "group_var = 'SK_ID_CURR'\n",
        "\n",
        "# Need to create new column names\n",
        "columns = []\n",
        "\n",
        "# Iterate through the variables names\n",
        "for var in categorical_grouped.columns.levels[0]:\n",
        "    # Skip the grouping variable\n",
        "    if var != group_var:\n",
        "        # Iterate through the stat names\n",
        "        for stat in ['count', 'count_norm']:\n",
        "            # Make a new column name for the variable and stat\n",
        "            columns.append('%s_%s' % (var, stat))\n",
        "\n",
        "#  Rename the columns\n",
        "categorical_grouped.columns = columns\n",
        "\n",
        "categorical_grouped.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:56:26.229317Z",
          "iopub.status.busy": "2023-11-15T18:56:26.228981Z",
          "iopub.status.idle": "2023-11-15T18:56:27.029470Z",
          "shell.execute_reply": "2023-11-15T18:56:27.028728Z",
          "shell.execute_reply.started": "2023-11-15T18:56:26.229258Z"
        },
        "id": "L049UhZupVpn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train = train.merge(categorical_grouped, left_on = 'SK_ID_CURR', right_index = True, how = 'left')\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:56:36.219514Z",
          "iopub.status.busy": "2023-11-15T18:56:36.219191Z",
          "iopub.status.idle": "2023-11-15T18:56:36.225411Z",
          "shell.execute_reply": "2023-11-15T18:56:36.224596Z",
          "shell.execute_reply.started": "2023-11-15T18:56:36.219471Z"
        },
        "id": "aWdtpjbnpVpo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:56:43.176794Z",
          "iopub.status.busy": "2023-11-15T18:56:43.176095Z",
          "iopub.status.idle": "2023-11-15T18:56:43.331530Z",
          "shell.execute_reply": "2023-11-15T18:56:43.330833Z",
          "shell.execute_reply.started": "2023-11-15T18:56:43.176735Z"
        },
        "id": "82RXYt49pVpo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train.iloc[:10, 123:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:56:56.373421Z",
          "iopub.status.busy": "2023-11-15T18:56:56.372735Z",
          "iopub.status.idle": "2023-11-15T18:56:56.380550Z",
          "shell.execute_reply": "2023-11-15T18:56:56.379821Z",
          "shell.execute_reply.started": "2023-11-15T18:56:56.373358Z"
        },
        "id": "3eVuGAwepVpo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def count_categorical(df, group_var, df_name):\n",
        "    \"\"\"Computes counts and normalized counts for each observation\n",
        "    of `group_var` of each unique category in every categorical variable\n",
        "\n",
        "    Parameters\n",
        "    --------\n",
        "    df : dataframe\n",
        "        The dataframe to calculate the value counts for.\n",
        "\n",
        "    group_var : string\n",
        "        The variable by which to group the dataframe. For each unique\n",
        "        value of this variable, the final dataframe will have one row\n",
        "\n",
        "    df_name : string\n",
        "        Variable added to the front of column names to keep track of columns\n",
        "\n",
        "\n",
        "    Return\n",
        "    --------\n",
        "    categorical : dataframe\n",
        "        A dataframe with counts and normalized counts of each unique category in every categorical variable\n",
        "        with one row for every unique value of the `group_var`.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Select the categorical columns\n",
        "    categorical = pd.get_dummies(df.select_dtypes('object'))\n",
        "\n",
        "    # Make sure to put the identifying id on the column\n",
        "    categorical[group_var] = df[group_var]\n",
        "\n",
        "    # Groupby the group var and calculate the sum and mean\n",
        "    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])\n",
        "\n",
        "    column_names = []\n",
        "\n",
        "    # Iterate through the columns in level 0\n",
        "    for var in categorical.columns.levels[0]:\n",
        "        # Iterate through the stats in level 1\n",
        "        for stat in ['count', 'count_norm']:\n",
        "            # Make a new column name\n",
        "            column_names.append('%s_%s_%s' % (df_name, var, stat))\n",
        "\n",
        "    categorical.columns = column_names\n",
        "\n",
        "    # Remove duplicate columns by values\n",
        "    _, idx = np.unique(categorical, axis = 1, return_index = True)\n",
        "    categorical = categorical.iloc[:, idx]\n",
        "\n",
        "    return categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def return_size(df):\n",
        "    \"\"\"Return size of dataframe in gigabytes\"\"\"\n",
        "    return round(sys.getsizeof(df) / 1e9, 2)\n",
        "\n",
        "def convert_types(df, print_info = False):\n",
        "    \n",
        "    original_memory = df.memory_usage().sum()\n",
        "    \n",
        "    # Iterate through each column\n",
        "    for c in df:\n",
        "        \n",
        "        # Convert ids and booleans to integers\n",
        "        if ('SK_ID' in c):\n",
        "            df[c] = df[c].fillna(0).astype(np.int32)\n",
        "            \n",
        "        # Convert objects to category\n",
        "        elif (df[c].dtype == 'object') and (df[c].nunique() < df.shape[0]):\n",
        "            df[c] = df[c].astype('category')\n",
        "        \n",
        "        # Booleans mapped to integers\n",
        "        elif list(df[c].unique()) == [1, 0]:\n",
        "            df[c] = df[c].astype(bool)\n",
        "        \n",
        "        # Float64 to float32\n",
        "        elif df[c].dtype == float:\n",
        "            df[c] = df[c].astype(np.float32)\n",
        "            \n",
        "        # Int64 to int32\n",
        "        elif df[c].dtype == int:\n",
        "            df[c] = df[c].astype(np.int32)\n",
        "        \n",
        "    new_memory = df.memory_usage().sum()\n",
        "    \n",
        "    if print_info:\n",
        "        print(f'Original Memory Usage: {round(original_memory / 1e9, 2)} gb.')\n",
        "        print(f'New Memory Usage: {round(new_memory / 1e9, 2)} gb.')\n",
        "        \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:57:03.740215Z",
          "iopub.status.busy": "2023-11-15T18:57:03.739553Z",
          "iopub.status.idle": "2023-11-15T18:57:07.243414Z",
          "shell.execute_reply": "2023-11-15T18:57:07.242450Z",
          "shell.execute_reply.started": "2023-11-15T18:57:03.740158Z"
        },
        "id": "x9Ba3MXRpVpo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "bureau_counts = count_categorical(bureau, group_var = 'SK_ID_CURR', df_name = 'bureau')\n",
        "bureau_counts.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:57:40.385642Z",
          "iopub.status.busy": "2023-11-15T18:57:40.385302Z",
          "iopub.status.idle": "2023-11-15T18:57:53.806515Z",
          "shell.execute_reply": "2023-11-15T18:57:53.805535Z",
          "shell.execute_reply.started": "2023-11-15T18:57:40.385590Z"
        },
        "id": "F8OG4xBppVpo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Read in bureau balance\n",
        "bureau_balance = pd.read_csv('/Users/singh.aditya2/Downloads/home-credit-default-risk/bureau_balance.csv')\n",
        "bureau_balance.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:57:53.808506Z",
          "iopub.status.busy": "2023-11-15T18:57:53.807947Z",
          "iopub.status.idle": "2023-11-15T18:58:13.119062Z",
          "shell.execute_reply": "2023-11-15T18:58:13.117867Z",
          "shell.execute_reply.started": "2023-11-15T18:57:53.808438Z"
        },
        "id": "kfx_ixm-pVpo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Counts of each type of status for each previous loan\n",
        "bureau_balance_counts = count_categorical(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\n",
        "bureau_balance_counts.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:58:28.205592Z",
          "iopub.status.busy": "2023-11-15T18:58:28.204968Z",
          "iopub.status.idle": "2023-11-15T18:58:44.348362Z",
          "shell.execute_reply": "2023-11-15T18:58:44.347411Z",
          "shell.execute_reply.started": "2023-11-15T18:58:28.205251Z"
        },
        "id": "dCItV8yRpVpo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Counts of each type of status for each previous loan\n",
        "bureau_balance_counts = count_categorical(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\n",
        "bureau_balance_counts.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:58:56.319298Z",
          "iopub.status.busy": "2023-11-15T18:58:56.318950Z",
          "iopub.status.idle": "2023-11-15T18:59:00.712420Z",
          "shell.execute_reply": "2023-11-15T18:59:00.711197Z",
          "shell.execute_reply.started": "2023-11-15T18:58:56.319232Z"
        },
        "id": "rlU1-rRKpVpo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Calculate value count statistics for each `SK_ID_CURR`\n",
        "bureau_balance_agg = agg_numeric(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\n",
        "bureau_balance_agg.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:59:05.275327Z",
          "iopub.status.busy": "2023-11-15T18:59:05.274798Z",
          "iopub.status.idle": "2023-11-15T18:59:06.413550Z",
          "shell.execute_reply": "2023-11-15T18:59:06.412454Z",
          "shell.execute_reply.started": "2023-11-15T18:59:05.275089Z"
        },
        "id": "ncft-JRdpVpo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Dataframe grouped by the loan\n",
        "bureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index = True, left_on = 'SK_ID_BUREAU', how = 'outer')\n",
        "\n",
        "# Merge to include the SK_ID_CURR\n",
        "bureau_by_loan = bureau_by_loan.merge(bureau[['SK_ID_BUREAU', 'SK_ID_CURR']], on = 'SK_ID_BUREAU', how = 'left')\n",
        "\n",
        "bureau_by_loan.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:59:13.605075Z",
          "iopub.status.busy": "2023-11-15T18:59:13.604737Z",
          "iopub.status.idle": "2023-11-15T18:59:16.853541Z",
          "shell.execute_reply": "2023-11-15T18:59:16.852381Z",
          "shell.execute_reply.started": "2023-11-15T18:59:13.605021Z"
        },
        "id": "B8P6KMv7pVpp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "bureau_balance_by_client = agg_numeric(bureau_by_loan.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'client')\n",
        "bureau_balance_by_client.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:59:27.027320Z",
          "iopub.status.busy": "2023-11-15T18:59:27.026880Z",
          "iopub.status.idle": "2023-11-15T18:59:27.265932Z",
          "shell.execute_reply": "2023-11-15T18:59:27.264863Z",
          "shell.execute_reply.started": "2023-11-15T18:59:27.027254Z"
        },
        "id": "1iczNJFLpVpp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Free up memory by deleting old objects\n",
        "import gc\n",
        "gc.enable()\n",
        "del train, bureau, bureau_balance, bureau_agg, bureau_agg_new, bureau_balance_agg, bureau_balance_counts, bureau_by_loan, bureau_balance_by_client, bureau_counts\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T18:59:38.405585Z",
          "iopub.status.busy": "2023-11-15T18:59:38.405243Z",
          "iopub.status.idle": "2023-11-15T18:59:55.503156Z",
          "shell.execute_reply": "2023-11-15T18:59:55.502131Z",
          "shell.execute_reply.started": "2023-11-15T18:59:38.405534Z"
        },
        "id": "42LNgEo9pVpp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Read in new copies of all the dataframes\n",
        "train = pd.read_csv('/Users/singh.aditya2/Downloads/home-credit-default-risk/application_train.csv')\n",
        "bureau = pd.read_csv('/Users/singh.aditya2/Downloads/home-credit-default-risk/bureau.csv')\n",
        "bureau_balance = pd.read_csv('/Users/singh.aditya2/Downloads/home-credit-default-risk/bureau_balance.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:00:17.083331Z",
          "iopub.status.busy": "2023-11-15T19:00:17.082592Z",
          "iopub.status.idle": "2023-11-15T19:00:20.628201Z",
          "shell.execute_reply": "2023-11-15T19:00:20.627204Z",
          "shell.execute_reply.started": "2023-11-15T19:00:17.083251Z"
        },
        "id": "ZrSuSZAfpVpp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "bureau_counts = count_categorical(bureau, group_var = 'SK_ID_CURR', df_name = 'bureau')\n",
        "bureau_counts.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:00:30.607254Z",
          "iopub.status.busy": "2023-11-15T19:00:30.606502Z",
          "iopub.status.idle": "2023-11-15T19:00:35.022477Z",
          "shell.execute_reply": "2023-11-15T19:00:35.021455Z",
          "shell.execute_reply.started": "2023-11-15T19:00:30.607169Z"
        },
        "id": "SlPtK6Q6pVpp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "bureau_agg = agg_numeric(bureau.drop(columns = ['SK_ID_BUREAU','CREDIT_ACTIVE', 'CREDIT_CURRENCY','CREDIT_TYPE']), group_var = 'SK_ID_CURR', df_name = 'bureau')\n",
        "bureau_agg.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:00:40.418906Z",
          "iopub.status.busy": "2023-11-15T19:00:40.418255Z",
          "iopub.status.idle": "2023-11-15T19:00:59.758258Z",
          "shell.execute_reply": "2023-11-15T19:00:59.757202Z",
          "shell.execute_reply.started": "2023-11-15T19:00:40.418837Z"
        },
        "id": "di-E6ZBbpVpp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "bureau_balance_counts = count_categorical(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\n",
        "bureau_balance_counts.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:00:59.759767Z",
          "iopub.status.busy": "2023-11-15T19:00:59.759482Z",
          "iopub.status.idle": "2023-11-15T19:01:04.121303Z",
          "shell.execute_reply": "2023-11-15T19:01:04.119967Z",
          "shell.execute_reply.started": "2023-11-15T19:00:59.759718Z"
        },
        "id": "vyLEI4ThpVpp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "bureau_balance_agg = agg_numeric(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\n",
        "bureau_balance_agg.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:01:04.123341Z",
          "iopub.status.busy": "2023-11-15T19:01:04.122937Z",
          "iopub.status.idle": "2023-11-15T19:01:14.466196Z",
          "shell.execute_reply": "2023-11-15T19:01:14.465130Z",
          "shell.execute_reply.started": "2023-11-15T19:01:04.123264Z"
        },
        "id": "MmXYyTKEpVpp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Dataframe grouped by the loan\n",
        "bureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index = True, left_on = 'SK_ID_BUREAU', how = 'outer')\n",
        "\n",
        "# Merge to include the SK_ID_CURR\n",
        "bureau_by_loan = bureau[['SK_ID_BUREAU', 'SK_ID_CURR']].merge(bureau_by_loan, on = 'SK_ID_BUREAU', how = 'left')\n",
        "\n",
        "# Aggregate the stats for each client\n",
        "bureau_balance_by_client = agg_numeric(bureau_by_loan.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'client')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:01:15.664361Z",
          "iopub.status.busy": "2023-11-15T19:01:15.664029Z",
          "iopub.status.idle": "2023-11-15T19:01:15.669425Z",
          "shell.execute_reply": "2023-11-15T19:01:15.668521Z",
          "shell.execute_reply.started": "2023-11-15T19:01:15.664303Z"
        },
        "id": "JPbogs_0pVpp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "original_features = list(train.columns)\n",
        "print('Original Number of Features: ', len(original_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:01:23.606923Z",
          "iopub.status.busy": "2023-11-15T19:01:23.606545Z",
          "iopub.status.idle": "2023-11-15T19:01:39.990837Z",
          "shell.execute_reply": "2023-11-15T19:01:39.989810Z",
          "shell.execute_reply.started": "2023-11-15T19:01:23.606861Z"
        },
        "id": "HkLbB1fCpVpp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Merge with the value counts of bureau\n",
        "train = train.merge(bureau_counts, on = 'SK_ID_CURR', how = 'left')\n",
        "\n",
        "# Merge with the stats of bureau\n",
        "train = train.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\n",
        "\n",
        "# Merge with the monthly information grouped by client\n",
        "train = train.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:01:39.993256Z",
          "iopub.status.busy": "2023-11-15T19:01:39.992844Z",
          "iopub.status.idle": "2023-11-15T19:01:39.999175Z",
          "shell.execute_reply": "2023-11-15T19:01:39.998351Z",
          "shell.execute_reply.started": "2023-11-15T19:01:39.993178Z"
        },
        "id": "AUbHhX07pVpp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "new_features = list(train.columns)\n",
        "print('Number of features using previous loans from other institutions data: ', len(new_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:01:53.505222Z",
          "iopub.status.busy": "2023-11-15T19:01:53.504636Z",
          "iopub.status.idle": "2023-11-15T19:02:00.021214Z",
          "shell.execute_reply": "2023-11-15T19:02:00.020331Z",
          "shell.execute_reply.started": "2023-11-15T19:01:53.505167Z"
        },
        "id": "CIqNf2RNpVpq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "missing_train = missing_values_table(train)\n",
        "missing_train.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:02:05.806247Z",
          "iopub.status.busy": "2023-11-15T19:02:05.805889Z",
          "iopub.status.idle": "2023-11-15T19:02:05.814982Z",
          "shell.execute_reply": "2023-11-15T19:02:05.813644Z",
          "shell.execute_reply.started": "2023-11-15T19:02:05.806195Z"
        },
        "id": "-W-6m45tpVpq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "missing_train_vars = list(missing_train.index[missing_train['% of Total Values'] > 90])\n",
        "len(missing_train_vars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:02:15.688066Z",
          "iopub.status.busy": "2023-11-15T19:02:15.687678Z",
          "iopub.status.idle": "2023-11-15T19:02:19.039229Z",
          "shell.execute_reply": "2023-11-15T19:02:19.037914Z",
          "shell.execute_reply.started": "2023-11-15T19:02:15.687988Z"
        },
        "id": "nNp1Im2ZpVpq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Read in the test dataframe\n",
        "test = pd.read_csv('/Users/singh.aditya2/Downloads/home-credit-default-risk/application_test.csv')\n",
        "\n",
        "# Merge with the value counts of bureau\n",
        "test = test.merge(bureau_counts, on = 'SK_ID_CURR', how = 'left')\n",
        "\n",
        "# Merge with the stats of bureau\n",
        "test = test.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\n",
        "\n",
        "# Merge with the value counts of bureau balance\n",
        "test = test.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:02:30.928484Z",
          "iopub.status.busy": "2023-11-15T19:02:30.928128Z",
          "iopub.status.idle": "2023-11-15T19:02:30.933602Z",
          "shell.execute_reply": "2023-11-15T19:02:30.932571Z",
          "shell.execute_reply.started": "2023-11-15T19:02:30.928420Z"
        },
        "id": "VCOoK7B4pVpq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print('Shape of Testing Data: ', test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:02:40.414869Z",
          "iopub.status.busy": "2023-11-15T19:02:40.414499Z",
          "iopub.status.idle": "2023-11-15T19:02:41.408403Z",
          "shell.execute_reply": "2023-11-15T19:02:41.407512Z",
          "shell.execute_reply.started": "2023-11-15T19:02:40.414795Z"
        },
        "id": "xxTG80D4pVpq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_labels = train['TARGET']\n",
        "\n",
        "# Align the dataframes, this will remove the 'TARGET' column\n",
        "train, test = train.align(test, join = 'inner', axis = 1)\n",
        "\n",
        "train['TARGET'] = train_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:02:48.854080Z",
          "iopub.status.busy": "2023-11-15T19:02:48.853755Z",
          "iopub.status.idle": "2023-11-15T19:02:48.859545Z",
          "shell.execute_reply": "2023-11-15T19:02:48.858497Z",
          "shell.execute_reply.started": "2023-11-15T19:02:48.854028Z"
        },
        "id": "4pdU8Mj3pVpq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print('Training Data Shape: ', train.shape)\n",
        "print('Testing Data Shape: ', test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:02:57.728287Z",
          "iopub.status.busy": "2023-11-15T19:02:57.727956Z",
          "iopub.status.idle": "2023-11-15T19:02:58.406383Z",
          "shell.execute_reply": "2023-11-15T19:02:58.405392Z",
          "shell.execute_reply.started": "2023-11-15T19:02:57.728245Z"
        },
        "id": "qLOWlX6PpVpq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "missing_test = missing_values_table(test)\n",
        "missing_test.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:03:06.082476Z",
          "iopub.status.busy": "2023-11-15T19:03:06.082170Z",
          "iopub.status.idle": "2023-11-15T19:03:06.091223Z",
          "shell.execute_reply": "2023-11-15T19:03:06.090092Z",
          "shell.execute_reply.started": "2023-11-15T19:03:06.082436Z"
        },
        "id": "89l1KhKqpVpq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "missing_test_vars = list(missing_test.index[missing_test['% of Total Values'] > 90])\n",
        "len(missing_test_vars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:03:14.281442Z",
          "iopub.status.busy": "2023-11-15T19:03:14.281114Z",
          "iopub.status.idle": "2023-11-15T19:03:14.286082Z",
          "shell.execute_reply": "2023-11-15T19:03:14.285243Z",
          "shell.execute_reply.started": "2023-11-15T19:03:14.281388Z"
        },
        "id": "5RcWXy9QpVpr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "missing_columns = list(set(missing_test_vars + missing_train_vars))\n",
        "print('There are %d columns with more than 90%% missing in either the training or testing data.' % len(missing_columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:03:22.078469Z",
          "iopub.status.busy": "2023-11-15T19:03:22.078102Z",
          "iopub.status.idle": "2023-11-15T19:03:23.073232Z",
          "shell.execute_reply": "2023-11-15T19:03:23.072044Z",
          "shell.execute_reply.started": "2023-11-15T19:03:22.078392Z"
        },
        "id": "uwlgGDCppVpr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Drop the missing columns\n",
        "train = train.drop(columns = missing_columns)\n",
        "test = test.drop(columns = missing_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:03:39.680768Z",
          "iopub.status.busy": "2023-11-15T19:03:39.680109Z",
          "iopub.status.idle": "2023-11-15T19:05:23.598164Z",
          "shell.execute_reply": "2023-11-15T19:05:23.597081Z",
          "shell.execute_reply.started": "2023-11-15T19:03:39.680696Z"
        },
        "id": "XMLJPufypVpr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train.to_csv('/Users/singh.aditya2/Downloads/home-credit-default-risk/train_bureau_raw.csv', index = False)\n",
        "test.to_csv('/Users/singh.aditya2/Downloads/home-credit-default-risk/test_bureau_raw.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = train.drop(columns = missing_columns)\n",
        "test = test.drop(columns = missing_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:05:23.600375Z",
          "iopub.status.busy": "2023-11-15T19:05:23.599978Z",
          "iopub.status.idle": "2023-11-15T19:07:46.284059Z",
          "shell.execute_reply": "2023-11-15T19:07:46.282872Z",
          "shell.execute_reply.started": "2023-11-15T19:05:23.600301Z"
        },
        "id": "Ccog2rvUpVpr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Calculate all correlations in dataframe\n",
        "corrs = train.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:07:46.285773Z",
          "iopub.status.busy": "2023-11-15T19:07:46.285481Z",
          "iopub.status.idle": "2023-11-15T19:07:46.302693Z",
          "shell.execute_reply": "2023-11-15T19:07:46.301575Z",
          "shell.execute_reply.started": "2023-11-15T19:07:46.285724Z"
        },
        "id": "4tfQAqXjpVpr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "corrs = corrs.sort_values('TARGET', ascending = False)\n",
        "\n",
        "# Ten most positive correlations\n",
        "pd.DataFrame(corrs['TARGET'].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:07:46.304592Z",
          "iopub.status.busy": "2023-11-15T19:07:46.304203Z",
          "iopub.status.idle": "2023-11-15T19:07:46.327369Z",
          "shell.execute_reply": "2023-11-15T19:07:46.326512Z",
          "shell.execute_reply.started": "2023-11-15T19:07:46.304526Z"
        },
        "id": "cM3ewY1ipVpr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(corrs['TARGET'].dropna().tail(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:21:18.696078Z",
          "iopub.status.busy": "2023-11-15T19:21:18.695673Z",
          "iopub.status.idle": "2023-11-15T19:21:19.184377Z",
          "shell.execute_reply": "2023-11-15T19:21:19.183337Z",
          "shell.execute_reply.started": "2023-11-15T19:21:18.696022Z"
        },
        "id": "UBzo9tDspVpr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "kde_target(var_name='bureau_CREDIT_ACTIVE_Active_count_norm', df=train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:21:49.986280Z",
          "iopub.status.busy": "2023-11-15T19:21:49.985922Z",
          "iopub.status.idle": "2023-11-15T19:21:50.081725Z",
          "shell.execute_reply": "2023-11-15T19:21:50.080706Z",
          "shell.execute_reply.started": "2023-11-15T19:21:49.986217Z"
        },
        "id": "2gz6ycz7pVpr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Collinear Variables\n",
        "\n",
        "# Set the threshold\n",
        "threshold = 0.8\n",
        "\n",
        "# Empty dictionary to hold correlated variables\n",
        "above_threshold_vars = {}\n",
        "\n",
        "# For each column, record the variables that are above the threshold\n",
        "for col in corrs:\n",
        "    above_threshold_vars[col] = list(corrs.index[corrs[col] > threshold])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:22:01.758769Z",
          "iopub.status.busy": "2023-11-15T19:22:01.758382Z",
          "iopub.status.idle": "2023-11-15T19:22:01.773198Z",
          "shell.execute_reply": "2023-11-15T19:22:01.771490Z",
          "shell.execute_reply.started": "2023-11-15T19:22:01.758704Z"
        },
        "id": "CXKIOqaEpVpr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Track columns to remove and columns already examined\n",
        "cols_to_remove = []\n",
        "cols_seen = []\n",
        "cols_to_remove_pair = []\n",
        "\n",
        "# Iterate through columns and correlated columns\n",
        "for key, value in above_threshold_vars.items():\n",
        "    # Keep track of columns already examined\n",
        "    cols_seen.append(key)\n",
        "    for x in value:\n",
        "        if x == key:\n",
        "            next\n",
        "        else:\n",
        "            # Only want to remove one in a pair\n",
        "            if x not in cols_seen:\n",
        "                cols_to_remove.append(x)\n",
        "                cols_to_remove_pair.append(key)\n",
        "\n",
        "cols_to_remove = list(set(cols_to_remove))\n",
        "print('Number of columns to remove: ', len(cols_to_remove))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:22:09.596251Z",
          "iopub.status.busy": "2023-11-15T19:22:09.595768Z",
          "iopub.status.idle": "2023-11-15T19:22:10.115340Z",
          "shell.execute_reply": "2023-11-15T19:22:10.114525Z",
          "shell.execute_reply.started": "2023-11-15T19:22:09.596203Z"
        },
        "id": "2X-teOc0pVpr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_corrs_removed = train.drop(columns = cols_to_remove)\n",
        "test_corrs_removed = test.drop(columns = cols_to_remove)\n",
        "\n",
        "print('Training Corrs Removed Shape: ', train_corrs_removed.shape)\n",
        "print('Testing Corrs Removed Shape: ', test_corrs_removed.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:22:18.249343Z",
          "iopub.status.busy": "2023-11-15T19:22:18.248841Z",
          "iopub.status.idle": "2023-11-15T19:23:19.963047Z",
          "shell.execute_reply": "2023-11-15T19:23:19.962153Z",
          "shell.execute_reply.started": "2023-11-15T19:22:18.249294Z"
        },
        "id": "BtgXVCIypVps",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_corrs_removed.to_csv('train_bureau_corrs_removed.csv', index = False)\n",
        "test_corrs_removed.to_csv('test_bureau_corrs_removed.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:23:19.967184Z",
          "iopub.status.busy": "2023-11-15T19:23:19.966899Z",
          "iopub.status.idle": "2023-11-15T19:23:19.972192Z",
          "shell.execute_reply": "2023-11-15T19:23:19.971192Z",
          "shell.execute_reply.started": "2023-11-15T19:23:19.967136Z"
        },
        "id": "HeNsnmZPpVps",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import gc\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:23:19.974329Z",
          "iopub.status.busy": "2023-11-15T19:23:19.973736Z",
          "iopub.status.idle": "2023-11-15T19:23:19.997560Z",
          "shell.execute_reply": "2023-11-15T19:23:19.996784Z",
          "shell.execute_reply.started": "2023-11-15T19:23:19.973998Z"
        },
        "id": "pljBNerHpVps",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def model(features, test_features, encoding = 'ohe', n_folds = 5):\n",
        "\n",
        "    \"\"\"Train and test a light gradient boosting model using\n",
        "    cross validation.\n",
        "\n",
        "    Parameters\n",
        "    --------\n",
        "        features (pd.DataFrame):\n",
        "            dataframe of training features to use\n",
        "            for training a model. Must include the TARGET column.\n",
        "        test_features (pd.DataFrame):\n",
        "            dataframe of testing features to use\n",
        "            for making predictions with the model.\n",
        "        encoding (str, default = 'ohe'):\n",
        "            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n",
        "            n_folds (int, default = 5): number of folds to use for cross validation\n",
        "\n",
        "    Return\n",
        "    --------\n",
        "        submission (pd.DataFrame):\n",
        "            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n",
        "            predicted by the model.\n",
        "        feature_importances (pd.DataFrame):\n",
        "            dataframe with the feature importances from the model.\n",
        "        valid_metrics (pd.DataFrame):\n",
        "            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract the ids\n",
        "    train_ids = features['SK_ID_CURR']\n",
        "    test_ids = test_features['SK_ID_CURR']\n",
        "\n",
        "    # Extract the labels for training\n",
        "    labels = features['TARGET']\n",
        "\n",
        "    # Remove the ids and target\n",
        "    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
        "    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n",
        "\n",
        "\n",
        "    # One Hot Encoding\n",
        "    if encoding == 'ohe':\n",
        "        features = pd.get_dummies(features)\n",
        "        test_features = pd.get_dummies(test_features)\n",
        "\n",
        "        # Align the dataframes by the columns\n",
        "        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n",
        "\n",
        "        # No categorical indices to record\n",
        "        cat_indices = 'auto'\n",
        "\n",
        "    # Integer label encoding\n",
        "    elif encoding == 'le':\n",
        "\n",
        "        # Create a label encoder\n",
        "        label_encoder = LabelEncoder()\n",
        "\n",
        "        # List for storing categorical indices\n",
        "        cat_indices = []\n",
        "\n",
        "        # Iterate through each column\n",
        "        for i, col in enumerate(features):\n",
        "            if features[col].dtype == 'object':\n",
        "                # Map the categorical features to integers\n",
        "                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n",
        "                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n",
        "\n",
        "                # Record the categorical indices\n",
        "                cat_indices.append(i)\n",
        "\n",
        "    # Catch error if label encoding scheme is not valid\n",
        "    else:\n",
        "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
        "\n",
        "    print('Training Data Shape: ', features.shape)\n",
        "    print('Testing Data Shape: ', test_features.shape)\n",
        "\n",
        "    # Extract feature names\n",
        "    feature_names = list(features.columns)\n",
        "\n",
        "    # Convert to np arrays\n",
        "    features = np.array(features)\n",
        "    test_features = np.array(test_features)\n",
        "\n",
        "    # Create the kfold object\n",
        "    k_fold = KFold(n_splits = n_folds, shuffle = False, random_state = 50)\n",
        "\n",
        "    # Empty array for feature importances\n",
        "    feature_importance_values = np.zeros(len(feature_names))\n",
        "\n",
        "    # Empty array for test predictions\n",
        "    test_predictions = np.zeros(test_features.shape[0])\n",
        "\n",
        "    # Empty array for out of fold validation predictions\n",
        "    out_of_fold = np.zeros(features.shape[0])\n",
        "\n",
        "    # Lists for recording validation and training scores\n",
        "    valid_scores = []\n",
        "    train_scores = []\n",
        "\n",
        "    # Iterate through each fold\n",
        "    for train_indices, valid_indices in k_fold.split(features):\n",
        "\n",
        "        # Training data for the fold\n",
        "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
        "        # Validation data for the fold\n",
        "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
        "\n",
        "        # Create the model\n",
        "        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary',\n",
        "                                   class_weight = 'balanced', learning_rate = 0.05,\n",
        "                                   reg_alpha = 0.1, reg_lambda = 0.1,\n",
        "                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
        "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
        "                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n",
        "                  early_stopping_rounds = 100, verbose = 200)\n",
        "\n",
        "        # Record the best iteration\n",
        "        best_iteration = model.best_iteration_\n",
        "\n",
        "        # Record the feature importances\n",
        "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
        "\n",
        "        # Make predictions\n",
        "        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n",
        "\n",
        "        # Record the out of fold predictions\n",
        "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
        "\n",
        "        # Record the best score\n",
        "        valid_score = model.best_score_['valid']['auc']\n",
        "        train_score = model.best_score_['train']['auc']\n",
        "\n",
        "        valid_scores.append(valid_score)\n",
        "        train_scores.append(train_score)\n",
        "\n",
        "        # Clean up memory\n",
        "        gc.enable()\n",
        "        del model, train_features, valid_features\n",
        "        gc.collect()\n",
        "\n",
        "    # Make the submission dataframe\n",
        "    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n",
        "\n",
        "    # Make the feature importance dataframe\n",
        "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
        "\n",
        "    # Overall validation score\n",
        "    valid_auc = roc_auc_score(labels, out_of_fold)\n",
        "\n",
        "    # Add the overall scores to the metrics\n",
        "    valid_scores.append(valid_auc)\n",
        "    train_scores.append(np.mean(train_scores))\n",
        "\n",
        "    # Needed for creating dataframe of validation scores\n",
        "    fold_names = list(range(n_folds))\n",
        "    fold_names.append('overall')\n",
        "\n",
        "    # Dataframe of validation scores\n",
        "    metrics = pd.DataFrame({'fold': fold_names,\n",
        "                            'train': train_scores,\n",
        "                            'valid': valid_scores})\n",
        "\n",
        "    return submission, feature_importances, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:23:29.442786Z",
          "iopub.status.busy": "2023-11-15T19:23:29.442273Z",
          "iopub.status.idle": "2023-11-15T19:23:29.450642Z",
          "shell.execute_reply": "2023-11-15T19:23:29.449591Z",
          "shell.execute_reply.started": "2023-11-15T19:23:29.442736Z"
        },
        "id": "U59ZPjtlpVps",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def plot_feature_importances(df):\n",
        "    \"\"\"\n",
        "    Plot importances returned by a model. This can work with any measure of\n",
        "    feature importance provided that higher importance is better.\n",
        "\n",
        "    Args:\n",
        "        df (dataframe): feature importances. Must have the features in a column\n",
        "        called `features` and the importances in a column called `importance\n",
        "\n",
        "    Returns:\n",
        "        shows a plot of the 15 most importance features\n",
        "\n",
        "        df (dataframe): feature importances sorted by importance (highest to lowest)\n",
        "        with a column for normalized importance\n",
        "        \"\"\"\n",
        "\n",
        "    # Sort features according to importance\n",
        "    df = df.sort_values('importance', ascending = False).reset_index()\n",
        "\n",
        "    # Normalize the feature importances to add up to one\n",
        "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
        "\n",
        "    # Make a horizontal bar chart of feature importances\n",
        "    plt.figure(figsize = (10, 6))\n",
        "    ax = plt.subplot()\n",
        "\n",
        "    # Need to reverse the index to plot most important on top\n",
        "    ax.barh(list(reversed(list(df.index[:15]))),\n",
        "            df['importance_normalized'].head(15),\n",
        "            align = 'center', edgecolor = 'k')\n",
        "\n",
        "    # Set the yticks and labels\n",
        "    ax.set_yticks(list(reversed(list(df.index[:15]))))\n",
        "    ax.set_yticklabels(df['feature'].head(15))\n",
        "\n",
        "    # Plot labeling\n",
        "    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n",
        "    plt.show()\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:23:40.205340Z",
          "iopub.status.busy": "2023-11-15T19:23:40.204959Z",
          "iopub.status.idle": "2023-11-15T19:23:46.756741Z",
          "shell.execute_reply": "2023-11-15T19:23:46.755204Z",
          "shell.execute_reply.started": "2023-11-15T19:23:40.205263Z"
        },
        "id": "4yTv1WBFpVps",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_control = pd.read_csv('application_train.csv')\n",
        "test_control = pd.read_csv('application_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:23:57.368888Z",
          "iopub.status.busy": "2023-11-15T19:23:57.368273Z",
          "iopub.status.idle": "2023-11-15T19:28:35.849911Z",
          "shell.execute_reply": "2023-11-15T19:28:35.849025Z",
          "shell.execute_reply.started": "2023-11-15T19:23:57.368809Z"
        },
        "id": "Sb5XmU2ypVps",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "submission, fi, metrics = model(train_control, test_control)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:28:35.852381Z",
          "iopub.status.busy": "2023-11-15T19:28:35.851757Z",
          "iopub.status.idle": "2023-11-15T19:28:35.872835Z",
          "shell.execute_reply": "2023-11-15T19:28:35.871863Z",
          "shell.execute_reply.started": "2023-11-15T19:28:35.852127Z"
        },
        "id": "z2W12WNMpVps",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:28:35.956831Z",
          "iopub.status.busy": "2023-11-15T19:28:35.956122Z",
          "iopub.status.idle": "2023-11-15T19:28:36.286816Z",
          "shell.execute_reply": "2023-11-15T19:28:36.285475Z",
          "shell.execute_reply.started": "2023-11-15T19:28:35.956758Z"
        },
        "id": "AHHJNiTRpVps",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fi_sorted = plot_feature_importances(fi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:28:36.289362Z",
          "iopub.status.busy": "2023-11-15T19:28:36.288855Z",
          "iopub.status.idle": "2023-11-15T19:28:36.476841Z",
          "shell.execute_reply": "2023-11-15T19:28:36.475835Z",
          "shell.execute_reply.started": "2023-11-15T19:28:36.289262Z"
        },
        "id": "sPPbFhs_pVpt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "submission.to_csv('/Users/singh.aditya2/Downloads/home-credit-default-risk/control.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:28:36.478483Z",
          "iopub.status.busy": "2023-11-15T19:28:36.478177Z",
          "iopub.status.idle": "2023-11-15T19:40:04.350195Z",
          "shell.execute_reply": "2023-11-15T19:40:04.349196Z",
          "shell.execute_reply.started": "2023-11-15T19:28:36.478425Z"
        },
        "id": "yO-xERYqpVpt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "submission_raw, fi_raw, metrics_raw = model(train, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:40:04.352047Z",
          "iopub.status.busy": "2023-11-15T19:40:04.351739Z",
          "iopub.status.idle": "2023-11-15T19:40:04.368960Z",
          "shell.execute_reply": "2023-11-15T19:40:04.368218Z",
          "shell.execute_reply.started": "2023-11-15T19:40:04.351986Z"
        },
        "id": "mmEZQNJmpVpt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "metrics_raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:40:04.370792Z",
          "iopub.status.busy": "2023-11-15T19:40:04.370484Z",
          "iopub.status.idle": "2023-11-15T19:40:04.692836Z",
          "shell.execute_reply": "2023-11-15T19:40:04.691711Z",
          "shell.execute_reply.started": "2023-11-15T19:40:04.370735Z"
        },
        "id": "blfItvzwpVpt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fi_raw_sorted = plot_feature_importances(fi_raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:40:04.694776Z",
          "iopub.status.busy": "2023-11-15T19:40:04.694419Z",
          "iopub.status.idle": "2023-11-15T19:40:04.705636Z",
          "shell.execute_reply": "2023-11-15T19:40:04.704778Z",
          "shell.execute_reply.started": "2023-11-15T19:40:04.694714Z"
        },
        "id": "u2st56NFpVpt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "top_100 = list(fi_raw_sorted['feature'])[:100]\n",
        "new_features = [x for x in top_100 if x not in list(fi['feature'])]\n",
        "\n",
        "print('%% of Top 100 Features created from the bureau data = %d.00' % len(new_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:40:04.707488Z",
          "iopub.status.busy": "2023-11-15T19:40:04.707171Z",
          "iopub.status.idle": "2023-11-15T19:40:04.902677Z",
          "shell.execute_reply": "2023-11-15T19:40:04.901604Z",
          "shell.execute_reply.started": "2023-11-15T19:40:04.707428Z"
        },
        "id": "qyMDnsf3pVpt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "submission_raw.to_csv('/Users/singh.aditya2/Downloads/home-credit-default-risk/test_one.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:40:04.904429Z",
          "iopub.status.busy": "2023-11-15T19:40:04.904157Z",
          "iopub.status.idle": "2023-11-15T19:47:28.048556Z",
          "shell.execute_reply": "2023-11-15T19:47:28.047677Z",
          "shell.execute_reply.started": "2023-11-15T19:40:04.904383Z"
        },
        "id": "yrQVurOPpVpt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Test -2 Dropping highly cooreleated features\n",
        "submission_corrs, fi_corrs, metrics_corr = model(train_corrs_removed, test_corrs_removed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:47:28.051013Z",
          "iopub.status.busy": "2023-11-15T19:47:28.050407Z",
          "iopub.status.idle": "2023-11-15T19:47:28.068588Z",
          "shell.execute_reply": "2023-11-15T19:47:28.067737Z",
          "shell.execute_reply.started": "2023-11-15T19:47:28.050901Z"
        },
        "id": "j4cu4znNpVpt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "metrics_corr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:47:28.071111Z",
          "iopub.status.busy": "2023-11-15T19:47:28.070322Z",
          "iopub.status.idle": "2023-11-15T19:47:28.391872Z",
          "shell.execute_reply": "2023-11-15T19:47:28.390984Z",
          "shell.execute_reply.started": "2023-11-15T19:47:28.070849Z"
        },
        "id": "TW89Oar4pVpt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fi_corrs_sorted = plot_feature_importances(fi_corrs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-15T19:47:28.393347Z",
          "iopub.status.busy": "2023-11-15T19:47:28.393090Z",
          "iopub.status.idle": "2023-11-15T19:47:28.581301Z",
          "shell.execute_reply": "2023-11-15T19:47:28.580108Z",
          "shell.execute_reply.started": "2023-11-15T19:47:28.393304Z"
        },
        "id": "TCDNJ-7PpVpu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "submission_corrs.to_csv('/Users/singh.aditya2/Downloads/home-credit-default-risk/test_two.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EquOnIT-pVpu"
      },
      "source": [
        "# Some More Feature Engineering and PCA\n",
        "\n",
        "### Data taken into account: \n",
        "    previous_application\n",
        "    POS_CASH_balance\n",
        "    installments_payments\n",
        "    credit_card_balance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLYjAsntxZmj"
      },
      "outputs": [],
      "source": [
        "previous = pd.read_csv('previous_application.csv')\n",
        "previous = convert_types(previous, print_info=True)\n",
        "previous.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate aggregate statistics for each numeric column\n",
        "previous_agg = agg_numeric(previous, 'SK_ID_CURR', 'previous')\n",
        "print('Previous aggregation shape: ', previous_agg.shape)\n",
        "previous_agg.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate value counts for each categorical column\n",
        "previous_counts = count_categorical(previous, 'SK_ID_CURR', 'previous')\n",
        "print('Previous counts shape: ', previous_counts.shape)\n",
        "previous_counts.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = pd.read_csv('application_train.csv')\n",
        "train = convert_types(train)\n",
        "test = pd.read_csv('application_test.csv')\n",
        "test = convert_types(test)\n",
        "\n",
        "# Merge in the previous information\n",
        "train = train.merge(previous_counts, on ='SK_ID_CURR', how = 'left')\n",
        "train = train.merge(previous_agg, on = 'SK_ID_CURR', how = 'left')\n",
        "\n",
        "test = test.merge(previous_counts, on ='SK_ID_CURR', how = 'left')\n",
        "test = test.merge(previous_agg, on = 'SK_ID_CURR', how = 'left')\n",
        "\n",
        "# Remove variables to free memory\n",
        "gc.enable()\n",
        "del previous, previous_agg, previous_counts\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# removing The missing values columns with more than 90% of data missing \n",
        "train, test = remove_missing_columns(train, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to aggregate Stats at the client level\n",
        "\n",
        "\n",
        "def aggregate_client(df, group_vars, df_names):\n",
        "    \"\"\"Aggregate a dataframe with data at the loan level \n",
        "    at the client level\n",
        "    \n",
        "    Args:\n",
        "        df (dataframe): data at the loan level\n",
        "        group_vars (list of two strings): grouping variables for the loan \n",
        "        and then the client (example ['SK_ID_PREV', 'SK_ID_CURR'])\n",
        "        names (list of two strings): names to call the resulting columns\n",
        "        (example ['cash', 'client'])\n",
        "        \n",
        "    Returns:\n",
        "        df_client (dataframe): aggregated numeric stats at the client level. \n",
        "        Each client will have a single row with all the numeric data aggregated\n",
        "    \"\"\"\n",
        "    \n",
        "    # Aggregate the numeric columns\n",
        "    df_agg = agg_numeric(df, parent_var = group_vars[0], df_name = df_names[0])\n",
        "    \n",
        "    # If there are categorical variables\n",
        "    if any(df.dtypes == 'category'):\n",
        "    \n",
        "        # Count the categorical columns\n",
        "        df_counts = agg_categorical(df, parent_var = group_vars[0], df_name = df_names[0])\n",
        "\n",
        "        # Merge the numeric and categorical\n",
        "        df_by_loan = df_counts.merge(df_agg, on = group_vars[0], how = 'outer')\n",
        "\n",
        "        gc.enable()\n",
        "        del df_agg, df_counts\n",
        "        gc.collect()\n",
        "\n",
        "        # Merge to get the client id in dataframe\n",
        "        df_by_loan = df_by_loan.merge(df[[group_vars[0], group_vars[1]]], on = group_vars[0], how = 'left')\n",
        "\n",
        "        # Remove the loan id\n",
        "        df_by_loan = df_by_loan.drop(columns = [group_vars[0]])\n",
        "\n",
        "        # Aggregate numeric stats by column\n",
        "        df_by_client = agg_numeric(df_by_loan, parent_var = group_vars[1], df_name = df_names[1])\n",
        "\n",
        "        \n",
        "    # No categorical variables\n",
        "    else:\n",
        "        # Merge to get the client id in dataframe\n",
        "        df_by_loan = df_agg.merge(df[[group_vars[0], group_vars[1]]], on = group_vars[0], how = 'left')\n",
        "        \n",
        "        gc.enable()\n",
        "        del df_agg\n",
        "        gc.collect()\n",
        "        \n",
        "        # Remove the loan id\n",
        "        df_by_loan = df_by_loan.drop(columns = [group_vars[0]])\n",
        "        \n",
        "        # Aggregate numeric stats by column\n",
        "        df_by_client = agg_numeric(df_by_loan, parent_var = group_vars[1], df_name = df_names[1])\n",
        "        \n",
        "    # Memory management\n",
        "    gc.enable()\n",
        "    del df, df_by_loan\n",
        "    gc.collect()\n",
        "\n",
        "    return df_by_client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monthly Cash \n",
        "\n",
        "cash = pd.read_csv('POS_CASH_balance.csv')\n",
        "cash = convert_types(cash, print_info=True)\n",
        "cash.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cash_by_client = aggregate_client(cash, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['cash', 'client'])\n",
        "cash_by_client.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Cash by Client Shape: ', cash_by_client.shape)\n",
        "train = train.merge(cash_by_client, on = 'SK_ID_CURR', how = 'left')\n",
        "test = test.merge(cash_by_client, on = 'SK_ID_CURR', how = 'left')\n",
        "\n",
        "gc.enable()\n",
        "del cash, cash_by_client\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train, test = remove_missing_columns(train, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monthly Credit Data\n",
        "\n",
        "credit = pd.read_csv('credit_card_balance.csv')\n",
        "credit = convert_types(credit, print_info = True)\n",
        "credit.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "credit_by_client = aggregate_client(credit, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['credit', 'client'])\n",
        "credit_by_client.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Credit by client shape: ', credit_by_client.shape)\n",
        "\n",
        "train = train.merge(credit_by_client, on = 'SK_ID_CURR', how = 'left')\n",
        "test = test.merge(credit_by_client, on = 'SK_ID_CURR', how = 'left')\n",
        "\n",
        "gc.enable()\n",
        "del credit, credit_by_client\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train, test = remove_missing_columns(train, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Installment Payments\n",
        "\n",
        "installments = pd.read_csv('installments_payments.csv')\n",
        "installments = convert_types(installments, print_info = True)\n",
        "installments.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "installments_by_client = aggregate_client(installments, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['installments', 'client'])\n",
        "installments_by_client.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Installments by client shape: ', installments_by_client.shape)\n",
        "\n",
        "train = train.merge(installments_by_client, on = 'SK_ID_CURR', how = 'left')\n",
        "test = test.merge(installments_by_client, on = 'SK_ID_CURR', how = 'left')\n",
        "\n",
        "gc.enable()\n",
        "del installments, installments_by_client\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train, test = remove_missing_columns(train, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Final Training Shape: ', train.shape)\n",
        "print('Final Testing Shape: ', test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'Final training size: {return_size(train)}')\n",
        "print(f'Final testing size: {return_size(test)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "submission, fi, metrics = model(train, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "submission.to_csv('submission_manualp2.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read in the Data\n",
        "\n",
        "train_bureau = pd.read_csv('train_bureau_raw.csv', nrows = 1000)\n",
        "test_bureau = pd.read_csv('test_bureau_raw.csv', nrows = 1000)\n",
        "\n",
        "train_previous = pd.read_csv('train_previous_raw.csv', nrows = 1000)\n",
        "test_previous = pd.read_csv('test_previous_raw.csv', nrows = 1000)\n",
        "\n",
        "# All columns in dataframes\n",
        "bureau_columns = list(train_bureau.columns)\n",
        "previous_columns = list(train_previous.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bureau only features \n",
        "\n",
        "bureau_features = list(set(bureau_columns) - set(previous_columns))\n",
        "\n",
        "# Previous only features\n",
        "previous_features = list(set(previous_columns) - set(bureau_columns))\n",
        "\n",
        "# Original features will be in both datasets\n",
        "original_features = list(set(previous_columns) & set(bureau_columns))\n",
        "\n",
        "print('There are %d original features.' % len(original_features))\n",
        "print('There are %d bureau and bureau balance features.' % len(bureau_features))\n",
        "print('There are %d previous Home Credit loan features.' % len(previous_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Combining data without creating duplicates\n",
        "\n",
        "train_labels = train_bureau['TARGET']\n",
        "previous_features.append('SK_ID_CURR')\n",
        "\n",
        "train_ids = train_bureau['SK_ID_CURR']\n",
        "test_ids = test_bureau['SK_ID_CURR']\n",
        "\n",
        "# Merge the dataframes avoiding duplicating columns by subsetting train_previous\n",
        "train = train_bureau.merge(train_previous[previous_features], on = 'SK_ID_CURR')\n",
        "test = test_bureau.merge(test_previous[previous_features], on = 'SK_ID_CURR')\n",
        "\n",
        "print('Training shape: ', train.shape)\n",
        "print('Testing shape: ', test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One hot encoding\n",
        "train = pd.get_dummies(train)\n",
        "test = pd.get_dummies(test)\n",
        "\n",
        "# Match the columns in the dataframes\n",
        "train, test = train.align(test, join = 'inner', axis = 1)\n",
        "print('Training shape: ', train.shape)\n",
        "print('Testing shape: ', test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols_with_id = [x for x in train.columns if 'SK_ID_CURR' in x]\n",
        "cols_with_bureau_id = [x for x in train.columns if 'SK_ID_BUREAU' in x]\n",
        "cols_with_previous_id = [x for x in train.columns if 'SK_ID_PREV' in x]\n",
        "print('There are %d columns that contain SK_ID_CURR' % len(cols_with_id))\n",
        "print('There are %d columns that contain SK_ID_BUREAU' % len(cols_with_bureau_id))\n",
        "print('There are %d columns that contain SK_ID_PREV' % len(cols_with_previous_id))\n",
        "\n",
        "train = train.drop(columns = cols_with_id)\n",
        "test = test.drop(columns = cols_with_id)\n",
        "print('Training shape: ', train.shape)\n",
        "print('Testing shape: ', test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Removing Correlated Features\n",
        "\n",
        "# Threshold for removing correlated variables\n",
        "threshold = 0.9\n",
        "\n",
        "# Absolute value correlation matrix\n",
        "corr_matrix = train.corr().abs()\n",
        "corr_matrix.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upper triangle of correlations\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "upper.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select columns with correlations above threshold\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
        "\n",
        "print('There are %d columns to remove.' % (len(to_drop)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop correlated Variables\n",
        "\n",
        "train = train.drop(columns = to_drop)\n",
        "test = test.drop(columns = to_drop)\n",
        "\n",
        "print('Training shape: ', train.shape)\n",
        "print('Testing shape: ', test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = pd.read_csv('m_train_combined.csv')\n",
        "test = pd.read_csv('m_test_combined.csv')\n",
        "\n",
        "\n",
        "print('Training set full shape: ', train.shape)\n",
        "print('Testing set full shape: ' , test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train missing values (in percent)\n",
        "train_missing = (train.isnull().sum() / len(train)).sort_values(ascending = False)\n",
        "train_missing.head()\n",
        "\n",
        "\n",
        "\n",
        "# Test missing values (in percent)\n",
        "test_missing = (test.isnull().sum() / len(test)).sort_values(ascending = False)\n",
        "test_missing.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify missing values above threshold\n",
        "train_missing = train_missing.index[train_missing > 0.75]\n",
        "test_missing = test_missing.index[test_missing > 0.75]\n",
        "\n",
        "all_missing = list(set(set(train_missing) | set(test_missing)))\n",
        "print('There are %d columns with more than 75%% missing values' % len(all_missing))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Need to save the labels because aligning will remove this column\n",
        "train_labels = train[\"TARGET\"]\n",
        "train_ids = train['SK_ID_CURR']\n",
        "test_ids = test['SK_ID_CURR']\n",
        "\n",
        "train = pd.get_dummies(train.drop(columns = all_missing))\n",
        "test = pd.get_dummies(test.drop(columns = all_missing))\n",
        "\n",
        "train, test = train.align(test, join = 'inner', axis = 1)\n",
        "\n",
        "print('Training set full shape: ', train.shape)\n",
        "print('Testing set full shape: ' , test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = train.drop(columns = ['SK_ID_CURR'])\n",
        "test = test.drop(columns = ['SK_ID_CURR'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Selection through feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify missing values above threshold\n",
        "train_missing = train_missing.index[train_missing > 0.75]\n",
        "test_missing = test_missing.index[test_missing > 0.75]\n",
        "\n",
        "all_missing = list(set(set(train_missing) | set(test_missing)))\n",
        "print('There are %d columns with more than 75%% missing values' % len(all_missing))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit the model twice to avoid overfitting\n",
        "for i in range(2):\n",
        "    \n",
        "    # Split into training and validation set\n",
        "    train_features, valid_features, train_y, valid_y = train_test_split(train, train_labels, test_size = 0.25, random_state = i)\n",
        "    \n",
        "    # Train using early stopping\n",
        "    model.fit(train_features, train_y, early_stopping_rounds=100, eval_set = [(valid_features, valid_y)], \n",
        "              eval_metric = 'auc', verbose = 200)\n",
        "    \n",
        "    # Record the feature importances\n",
        "    feature_importances += model.feature_importances_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make sure to average feature importances! \n",
        "feature_importances = feature_importances / 2\n",
        "feature_importances = pd.DataFrame({'feature': list(train.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)\n",
        "\n",
        "feature_importances.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the features with zero importance\n",
        "zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\n",
        "print('There are %d features with 0.0 importance' % len(zero_features))\n",
        "feature_importances.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "norm_feature_importances = plot_feature_importances(feature_importances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = train.drop(columns = zero_features)\n",
        "test = test.drop(columns = zero_features)\n",
        "\n",
        "print('Training shape: ', train.shape)\n",
        "print('Testing shape: ', test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def identify_zero_importance_features(train, train_labels, iterations = 2):\n",
        "    \"\"\"\n",
        "    Identify zero importance features in a training dataset based on the \n",
        "    feature importances from a gradient boosting model. \n",
        "    \n",
        "    Parameters\n",
        "    --------\n",
        "    train : dataframe\n",
        "        Training features\n",
        "        \n",
        "    train_labels : np.array\n",
        "        Labels for training data\n",
        "        \n",
        "    iterations : integer, default = 2\n",
        "        Number of cross validation splits to use for determining feature importances\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize an empty array to hold feature importances\n",
        "    feature_importances = np.zeros(train.shape[1])\n",
        "\n",
        "    # Create the model with several hyperparameters\n",
        "    model = lgb.LGBMClassifier(objective='binary', boosting_type = 'goss', n_estimators = 10000, class_weight = 'balanced')\n",
        "    \n",
        "    # Fit the model multiple times to avoid overfitting\n",
        "    for i in range(iterations):\n",
        "\n",
        "        # Split into training and validation set\n",
        "        train_features, valid_features, train_y, valid_y = train_test_split(train, train_labels, test_size = 0.25, random_state = i)\n",
        "\n",
        "        # Train using early stopping\n",
        "        model.fit(train_features, train_y, early_stopping_rounds=100, eval_set = [(valid_features, valid_y)], \n",
        "                  eval_metric = 'auc', verbose = 200)\n",
        "\n",
        "        # Record the feature importances\n",
        "        feature_importances += model.feature_importances_ / iterations\n",
        "    \n",
        "    feature_importances = pd.DataFrame({'feature': list(train.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)\n",
        "    \n",
        "    # Find the features with zero importance\n",
        "    zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\n",
        "    print('\\nThere are %d features with 0.0 importance' % len(zero_features))\n",
        "    \n",
        "    return zero_features, feature_importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "second_round_zero_features, feature_importances = identify_zero_importance_features(train, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "norm_feature_importances = plot_feature_importances(feature_importances, threshold = 0.95)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Threshold for cumulative importance\n",
        "threshold = 0.95\n",
        "\n",
        "# Extract the features to keep\n",
        "features_to_keep = list(norm_feature_importances[norm_feature_importances['cumulative_importance'] < threshold]['feature'])\n",
        "\n",
        "# Create new datasets with smaller features\n",
        "train_small = train[features_to_keep]\n",
        "test_small = test[features_to_keep]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_small['TARGET'] = train_labels\n",
        "train_small['SK_ID_CURR'] = train_ids\n",
        "test_small['SK_ID_CURR'] = test_ids\n",
        "\n",
        "train_small.to_csv('m_train_small.csv', index = False)\n",
        "test_small.to_csv('m_test_small.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify missing values above threshold\n",
        "train_missing = train_missing.index[train_missing > 0.75]\n",
        "test_missing = test_missing.index[test_missing > 0.75]\n",
        "\n",
        "all_missing = list(set(set(train_missing) | set(test_missing)))\n",
        "print('There are %d columns with more than 75%% missing values' % len(all_missing))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "submission.to_csv('selected_features_submission.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "submission_small, feature_importances_small, metrics_small = model(train_small, test_small)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "submission_small.to_csv('selected_features_small_submission.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dimentionality Reduction\n",
        "\n",
        "##### PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import Imputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Make sure to drop the ids and target\n",
        "train = train.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
        "test = test.drop(columns = ['SK_ID_CURR'])\n",
        "\n",
        "# Make a pipeline with imputation and pca\n",
        "pipeline = Pipeline(steps = [('imputer', Imputer(strategy = 'median')),\n",
        "             ('pca', PCA())])\n",
        "\n",
        "# Fit and transform on the training data\n",
        "train_pca = pipeline.fit_transform(train)\n",
        "\n",
        "# transform the testing data\n",
        "test_pca = pipeline.transform(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the pca object\n",
        "pca = pipeline.named_steps['pca']\n",
        "\n",
        "# Plot the cumulative variance explained\n",
        "\n",
        "plt.figure(figsize = (10, 8))\n",
        "plt.plot(list(range(train.shape[1])), np.cumsum(pca.explained_variance_ratio_), 'r-')\n",
        "plt.xlabel('Number of PC'); plt.ylabel('Cumulative Variance Explained');\n",
        "plt.title('Cumulative Variance Explained with PCA');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataframe of pca results\n",
        "pca_df = pd.DataFrame({'pc_1': train_pca[:, 0], 'pc_2': train_pca[:, 1], 'target': train_labels})\n",
        "\n",
        "# Plot pc2 vs pc1 colored by target\n",
        "sns.lmplot('pc_1', 'pc_2', data = pca_df, hue = 'target', fit_reg=False, size = 10)\n",
        "plt.title('PC2 vs PC1 by Target')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('2 principal components account for {:.4f}% of the variance.'.format(100 * np.sum(pca.explained_variance_ratio_[:2])))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 860599,
          "sourceId": 9120,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 9432,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
